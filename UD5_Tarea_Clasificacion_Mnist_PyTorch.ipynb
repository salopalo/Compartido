{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:skyblue\"><u>Práctica UD5 N.N. con dataset Mnist</u></span>\n",
    "\n",
    "Salvador Lopez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:cornflowerblue\"><u>1.-Crea un modelo de clasificación del dataset MNIST con PyTorch con redes neuronales sin capas convolucionales, intentando mejorar todo lo posible su exactitud.</u> </span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.- Importamos las librerias necesarias\n",
    "\n",
    "Se importan las librerias de PyTorch necesarias para construir la Red Neuronal. \n",
    "\n",
    "Se descarga el dataset Mnist separando el grupo de entrenamiento y test y transformando las imagenes a un Tensor de PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: ToTensor() \n",
      "\n",
      " torch.Size([60000, 28, 28]) \n",
      "\n",
      " tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8) \n",
      "\n",
      " tensor([5, 0, 4,  ..., 5, 6, 8])\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "training_data = datasets.MNIST( # Crea un objeto MNIST para entrenamiento (subclase de torch.utils.data.Dataset)\n",
    "    root=\"data\", # ruta donde se almacenan los datos\n",
    "    train=True, # carga el conjunto de entrenamiento\n",
    "    download=True,  # descarga el conjunto de datos si no está en el directorio de datos\n",
    "    transform=ToTensor(), # ToTensor convierte la imagen en un tensor de PyTorch\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST( # Crea un objeto MNIST para Test\n",
    "    root=\"data\", # ruta donde se guardarán los datos\n",
    "    train=False, # no carga el conjunto de entrenamiento, sino de Test\n",
    "    download=True, # descarga el conjunto de datos si es necesario\n",
    "    transform=ToTensor(), # transforma la imagen en un tensor de PyTorch\n",
    ")\n",
    "\n",
    "print (training_data,\"\\n\\n\", training_data.data.shape, \"\\n\\n\", training_data.data, \"\\n\\n\", training_data.targets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.- Creamos los lotes necesarios para agilizar el procesado del dataset. \n",
    "\n",
    "En este caso cargamos el dataset en el DataLoader con un tamaño de lote o Batch de 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape de X [N(numero de muestras), C(canales de color), H(altura), W(anchura)]: torch.Size([64, 1, 28, 28])\n",
      "Shape de y: torch.Size([64]) torch.int64\n",
      "Shape de X [N(numero de muestras), C(canales de color), H(altura), W(anchura)]: torch.Size([64, 1, 28, 28])\n",
      "Shape de y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape de X [N(numero de muestras), C(canales de color), H(altura), W(anchura)]: {X.shape}\")\n",
    "    print(f\"Shape de y: {y.shape} {y.dtype}\")\n",
    "    break\n",
    "\n",
    "for X, y in train_dataloader:\n",
    "    print(f\"Shape de X [N(numero de muestras), C(canales de color), H(altura), W(anchura)]: {X.shape}\")\n",
    "    print(f\"Shape de y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.- Definicion de la Red Neuronal\n",
    "\n",
    "Definimos la clase a la que pertenecerá nuestro modelo mediante los métodos \"constructor (init)\" y \"forward\"y de la clase nn.Module, detallando los parámetros que vamos a incluir en nuestro modelo de Red Neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NeuralNetwork(nn.Module): # Clase que hereda de nn.Module y define la arquitectura de la red\n",
    "    def __init__(self): # Constructor de la clase\n",
    "        super().__init__() # Llama al constructor de la clase padre\n",
    "        self.flatten = nn.Flatten() # Capa de aplanamiento de la imagen (28x28 -> 784)\n",
    "        self.linear_relu_stack = nn.Sequential( # Secuencia de capas lineales y funciones de activación ReLU\n",
    "            nn.Linear(28*28, 512), # Capa de entrada con 784 entradas y 512 salidas\n",
    "            nn.ReLU(), # Función de activación ReLU después de la capa de entrada\n",
    "            nn.Linear(512, 512), # Capa oculta totalmente conectada con 512 entradas y 512 salidas\n",
    "            nn.ReLU(), # Función de activación ReLU después de la capa oculta\n",
    "            nn.Linear(512, 10) # Capa de salida con 512 entradas y 10 salidas\n",
    "        )\n",
    "\n",
    "    def forward(self, x): # Método que define el flujo de datos a través de la red\n",
    "        x = self.flatten(x) # Aplana la imagen\n",
    "        logits = self.linear_relu_stack(x) # Pasa los datos a través de la secuencia de capas\n",
    "        return logits # Devuelve los logits (salida sin activación)\n",
    "    \n",
    "model = NeuralNetwork() # Instancia del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4.- Optimización y entrenamiento\n",
    "\n",
    "### Definimos el Optimizador y la Función de Pérdida para nuestro modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss_fn = nn.CrossEntropyLoss() # Función de pérdida\n",
    "optimizer = torch.optim.SGD( # Optimizador de descenso de gradiente estocástico\n",
    "    model.parameters(), # Parámetros del modelo a optimizar\n",
    "    lr=0.001 # Tasa de aprendizaje\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definimos el método Train para entrenar nuestro modelo\n",
    "\n",
    "Para cada lote el método itera sobre los datos del lote o batch, establece una predicción, calcula la pérdida y activa el backpropagation reseteando los gradientes, calculando el nuevo gradiente de la función de pérdida y actualizando los parametros de peso y coste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available() \n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\" \n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "model = model.to(device) # Mueve el modelo a la GPU si está disponible\n",
    "print(model)\n",
    "\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    \n",
    "    size = len(dataloader.dataset) # Número de muestras en el conjunto de datos\n",
    "    \n",
    "    model.train() # Pone el modelo en modo de entrenamiento\n",
    "    for batch_num, (X, y) in enumerate(dataloader): # Itera sobre los lotes de datos, para cada uno:\n",
    "        X, y = X.to(), y.to(device) # Mueve el array de datos y las etiquetas al dispositivo\n",
    "\n",
    "        pred = model(X) # Genera predicciones\n",
    "        loss = loss_fn(pred, y) # Calcula la pérdida para ese lote\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad() # Resetea los gradientes\n",
    "        loss.backward() # Calcula el gradiente de la función de pérdida\n",
    "        optimizer.step() # Actualiza los parámetros\n",
    "\n",
    "        if batch_num % 100 == 0: # Cada 100 lotes imprime el progreso\n",
    "            loss, current = loss.item(), (batch_num + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definimos el metodo Test para evaluar nuestro modelo en el conjunto de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval() # Pone el modelo en modo de evaluación\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad(): # Desactiva el cálculo de gradientes para el siguiente bloque de código\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item() # Acumula la pérdida\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item() # Acumula el número de aciertos [1]\n",
    "    test_loss /= num_batches # Calcula la pérdida promedio por lote\n",
    "    correct /= size # Calcula la exactitud (número de aciertos / número total de muestras)\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenando nuestro modelo\n",
    "\n",
    "Definimos los \"Epochs\" o iteraciones que queremos que el modelo realice para su entrenamiento. Para cada Epoch se recorre el conjunto de datos dividido el lotes, realizando el ajuste de parámetros según lo definido en el BackPropagation del método Train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.305285  [   64/60000]\n",
      "loss: 2.299359  [ 6464/60000]\n",
      "loss: 2.294745  [12864/60000]\n",
      "loss: 2.285345  [19264/60000]\n",
      "loss: 2.287533  [25664/60000]\n",
      "loss: 2.289736  [32064/60000]\n",
      "loss: 2.283995  [38464/60000]\n",
      "loss: 2.284653  [44864/60000]\n",
      "loss: 2.272384  [51264/60000]\n",
      "loss: 2.256912  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 2.264708 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.265451  [   64/60000]\n",
      "loss: 2.255053  [ 6464/60000]\n",
      "loss: 2.257751  [12864/60000]\n",
      "loss: 2.232626  [19264/60000]\n",
      "loss: 2.244242  [25664/60000]\n",
      "loss: 2.244771  [32064/60000]\n",
      "loss: 2.231841  [38464/60000]\n",
      "loss: 2.243934  [44864/60000]\n",
      "loss: 2.220022  [51264/60000]\n",
      "loss: 2.200363  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 54.6%, Avg loss: 2.208418 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.211216  [   64/60000]\n",
      "loss: 2.191802  [ 6464/60000]\n",
      "loss: 2.204649  [12864/60000]\n",
      "loss: 2.153309  [19264/60000]\n",
      "loss: 2.178839  [25664/60000]\n",
      "loss: 2.174572  [32064/60000]\n",
      "loss: 2.149141  [38464/60000]\n",
      "loss: 2.176786  [44864/60000]\n",
      "loss: 2.134419  [51264/60000]\n",
      "loss: 2.105013  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.0%, Avg loss: 2.114102 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.119076  [   64/60000]\n",
      "loss: 2.084635  [ 6464/60000]\n",
      "loss: 2.113706  [12864/60000]\n",
      "loss: 2.019133  [19264/60000]\n",
      "loss: 2.063827  [25664/60000]\n",
      "loss: 2.051452  [32064/60000]\n",
      "loss: 2.006014  [38464/60000]\n",
      "loss: 2.057149  [44864/60000]\n",
      "loss: 1.985888  [51264/60000]\n",
      "loss: 1.939033  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 1.948538 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.958960  [   64/60000]\n",
      "loss: 1.898096  [ 6464/60000]\n",
      "loss: 1.950103  [12864/60000]\n",
      "loss: 1.798390  [19264/60000]\n",
      "loss: 1.860403  [25664/60000]\n",
      "loss: 1.836551  [32064/60000]\n",
      "loss: 1.769449  [38464/60000]\n",
      "loss: 1.853883  [44864/60000]\n",
      "loss: 1.744537  [51264/60000]\n",
      "loss: 1.676993  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.6%, Avg loss: 1.681693 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.707707  [   64/60000]\n",
      "loss: 1.608965  [ 6464/60000]\n",
      "loss: 1.682985  [12864/60000]\n",
      "loss: 1.491955  [19264/60000]\n",
      "loss: 1.547625  [25664/60000]\n",
      "loss: 1.519368  [32064/60000]\n",
      "loss: 1.446665  [38464/60000]\n",
      "loss: 1.565646  [44864/60000]\n",
      "loss: 1.435536  [51264/60000]\n",
      "loss: 1.357255  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 1.349200 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.402491  [   64/60000]\n",
      "loss: 1.270155  [ 6464/60000]\n",
      "loss: 1.354335  [12864/60000]\n",
      "loss: 1.177669  [19264/60000]\n",
      "loss: 1.209844  [25664/60000]\n",
      "loss: 1.186569  [32064/60000]\n",
      "loss: 1.128720  [38464/60000]\n",
      "loss: 1.270713  [44864/60000]\n",
      "loss: 1.159718  [51264/60000]\n",
      "loss: 1.083468  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.0%, Avg loss: 1.064674 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.142354  [   64/60000]\n",
      "loss: 0.997232  [ 6464/60000]\n",
      "loss: 1.076293  [12864/60000]\n",
      "loss: 0.946397  [19264/60000]\n",
      "loss: 0.959423  [25664/60000]\n",
      "loss: 0.940008  [32064/60000]\n",
      "loss: 0.896536  [38464/60000]\n",
      "loss: 1.047579  [44864/60000]\n",
      "loss: 0.969741  [51264/60000]\n",
      "loss: 0.898526  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.2%, Avg loss: 0.873177 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.964204  [   64/60000]\n",
      "loss: 0.818575  [ 6464/60000]\n",
      "loss: 0.884592  [12864/60000]\n",
      "loss: 0.796709  [19264/60000]\n",
      "loss: 0.797833  [25664/60000]\n",
      "loss: 0.780097  [32064/60000]\n",
      "loss: 0.742078  [38464/60000]\n",
      "loss: 0.896590  [44864/60000]\n",
      "loss: 0.843858  [51264/60000]\n",
      "loss: 0.779247  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.747922 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.844213  [   64/60000]\n",
      "loss: 0.700763  [ 6464/60000]\n",
      "loss: 0.754901  [12864/60000]\n",
      "loss: 0.698439  [19264/60000]\n",
      "loss: 0.689827  [25664/60000]\n",
      "loss: 0.675166  [32064/60000]\n",
      "loss: 0.635451  [38464/60000]\n",
      "loss: 0.793634  [44864/60000]\n",
      "loss: 0.754954  [51264/60000]\n",
      "loss: 0.699734  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.0%, Avg loss: 0.661920 \n",
      "\n",
      "Fin\n"
     ]
    }
   ],
   "source": [
    "epochs = 10 # Número de epochs\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Fin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comprobamos que en este modelo, con 1 capa de entrada, una capa oculta totalmente conectada de 512 neuronas, y una capa de salida, dividiendo el dataset en lotes de 64 muestras, y con una tasa de aprendizaje de 0.001, tras realizar 10 iteraciones de entrenamiento, la exactitud conseguida es del 84%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modificamos los valores de la red neuronal, modificando la capa oculta lineal por una con 512 entradas y 64 salidas, otra con 64 entradas y 128 salidas y otra más con 128 entradas y 512 salidas, para conectar con la capa de salida de 512 entradas y 10 salidas. Modificamos el Learning Rate a 0.01. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork_2(nn.Module): # Clase que hereda de nn.Module y define la arquitectura de la red\n",
    "    def __init__(self): # Constructor de la clase\n",
    "        super().__init__() # Llama al constructor de la clase padre\n",
    "        self.flatten = nn.Flatten() # Capa de aplanamiento de la imagen (28x28 -> 784)\n",
    "        self.linear_relu_stack = nn.Sequential( # Secuencia de capas lineales y funciones de activación ReLU\n",
    "            nn.Linear(28*28, 512), # Capa de entrada con 784 entradas y 512 salidas\n",
    "            nn.ReLU(), # Función de activación ReLU después de la capa de entrada\n",
    "            nn.Linear(512, 64), # Capa oculta con 512 entradas y 64 salidas\n",
    "            nn.ReLU(), # Función de activación ReLU después de la capa oculta\n",
    "            nn.Linear(64, 128), # Capa oculta con 64 entradas y 128 salidas\n",
    "            nn.ReLU(), # Función de activación ReLU después de la capa oculta\n",
    "            nn.Linear(128, 512), # Capa oculta con 128 entradas y 512 salidas\n",
    "            nn.ReLU(), # Función de activación ReLU después de la capa oculta\n",
    "            nn.Linear(512, 10) # Capa de salida con 512 entradas y 10 salidas\n",
    "        )\n",
    "\n",
    "    def forward(self, x): # Método que define el flujo de datos a través de la red\n",
    "        x = self.flatten(x) # Aplana la imagen\n",
    "        logits = self.linear_relu_stack(x) # Pasa los datos a través de la secuencia de capas\n",
    "        return logits # Devuelve los logits (salida sin activación)\n",
    "    \n",
    "model_2 = NeuralNetwork_2() # Instancia del modelo\n",
    "\n",
    "optimizer = torch.optim.SGD( # Optimizador de descenso de gradiente estocástico\n",
    "    model_2.parameters(), # Parámetros del modelo a optimizar\n",
    "    lr=0.001 # Tasa de aprendizaje\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.299796  [   64/60000]\n",
      "loss: 2.299484  [ 6464/60000]\n",
      "loss: 2.309227  [12864/60000]\n",
      "loss: 2.298249  [19264/60000]\n",
      "loss: 2.300488  [25664/60000]\n",
      "loss: 2.301455  [32064/60000]\n",
      "loss: 2.299978  [38464/60000]\n",
      "loss: 2.312638  [44864/60000]\n",
      "loss: 2.300023  [51264/60000]\n",
      "loss: 2.297873  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 13.6%, Avg loss: 2.300479 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.297742  [   64/60000]\n",
      "loss: 2.297847  [ 6464/60000]\n",
      "loss: 2.306180  [12864/60000]\n",
      "loss: 2.296951  [19264/60000]\n",
      "loss: 2.300315  [25664/60000]\n",
      "loss: 2.300670  [32064/60000]\n",
      "loss: 2.296892  [38464/60000]\n",
      "loss: 2.310769  [44864/60000]\n",
      "loss: 2.298865  [51264/60000]\n",
      "loss: 2.294949  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 11.3%, Avg loss: 2.298427 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.295634  [   64/60000]\n",
      "loss: 2.296136  [ 6464/60000]\n",
      "loss: 2.303362  [12864/60000]\n",
      "loss: 2.295328  [19264/60000]\n",
      "loss: 2.299688  [25664/60000]\n",
      "loss: 2.299629  [32064/60000]\n",
      "loss: 2.293842  [38464/60000]\n",
      "loss: 2.308831  [44864/60000]\n",
      "loss: 2.297395  [51264/60000]\n",
      "loss: 2.291950  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 11.3%, Avg loss: 2.296166 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.293289  [   64/60000]\n",
      "loss: 2.294052  [ 6464/60000]\n",
      "loss: 2.300545  [12864/60000]\n",
      "loss: 2.293130  [19264/60000]\n",
      "loss: 2.298508  [25664/60000]\n",
      "loss: 2.298132  [32064/60000]\n",
      "loss: 2.290641  [38464/60000]\n",
      "loss: 2.306573  [44864/60000]\n",
      "loss: 2.295388  [51264/60000]\n",
      "loss: 2.288688  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 11.3%, Avg loss: 2.293511 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2.290550  [   64/60000]\n",
      "loss: 2.291494  [ 6464/60000]\n",
      "loss: 2.297511  [12864/60000]\n",
      "loss: 2.290197  [19264/60000]\n",
      "loss: 2.296640  [25664/60000]\n",
      "loss: 2.296001  [32064/60000]\n",
      "loss: 2.287062  [38464/60000]\n",
      "loss: 2.303811  [44864/60000]\n",
      "loss: 2.292641  [51264/60000]\n",
      "loss: 2.285012  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 11.3%, Avg loss: 2.290217 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 2.287146  [   64/60000]\n",
      "loss: 2.288146  [ 6464/60000]\n",
      "loss: 2.294064  [12864/60000]\n",
      "loss: 2.286299  [19264/60000]\n",
      "loss: 2.293855  [25664/60000]\n",
      "loss: 2.292972  [32064/60000]\n",
      "loss: 2.282692  [38464/60000]\n",
      "loss: 2.300220  [44864/60000]\n",
      "loss: 2.288807  [51264/60000]\n",
      "loss: 2.280431  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 11.4%, Avg loss: 2.285892 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 2.282668  [   64/60000]\n",
      "loss: 2.283635  [ 6464/60000]\n",
      "loss: 2.289799  [12864/60000]\n",
      "loss: 2.281034  [19264/60000]\n",
      "loss: 2.289649  [25664/60000]\n",
      "loss: 2.288684  [32064/60000]\n",
      "loss: 2.276972  [38464/60000]\n",
      "loss: 2.295344  [44864/60000]\n",
      "loss: 2.283393  [51264/60000]\n",
      "loss: 2.274382  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 14.0%, Avg loss: 2.279975 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 2.276554  [   64/60000]\n",
      "loss: 2.277236  [ 6464/60000]\n",
      "loss: 2.284205  [12864/60000]\n",
      "loss: 2.273674  [19264/60000]\n",
      "loss: 2.283446  [25664/60000]\n",
      "loss: 2.282459  [32064/60000]\n",
      "loss: 2.269074  [38464/60000]\n",
      "loss: 2.288477  [44864/60000]\n",
      "loss: 2.275675  [51264/60000]\n",
      "loss: 2.266078  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 20.0%, Avg loss: 2.271624 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 2.267990  [   64/60000]\n",
      "loss: 2.268082  [ 6464/60000]\n",
      "loss: 2.276608  [12864/60000]\n",
      "loss: 2.263167  [19264/60000]\n",
      "loss: 2.274232  [25664/60000]\n",
      "loss: 2.273309  [32064/60000]\n",
      "loss: 2.257862  [38464/60000]\n",
      "loss: 2.278549  [44864/60000]\n",
      "loss: 2.264341  [51264/60000]\n",
      "loss: 2.254144  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 27.8%, Avg loss: 2.259493 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 2.255567  [   64/60000]\n",
      "loss: 2.254748  [ 6464/60000]\n",
      "loss: 2.265645  [12864/60000]\n",
      "loss: 2.247721  [19264/60000]\n",
      "loss: 2.260409  [25664/60000]\n",
      "loss: 2.259647  [32064/60000]\n",
      "loss: 2.241431  [38464/60000]\n",
      "loss: 2.263813  [44864/60000]\n",
      "loss: 2.247316  [51264/60000]\n",
      "loss: 2.236248  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 2.241265 \n",
      "\n",
      "Fin\n"
     ]
    }
   ],
   "source": [
    "epochs = 10 # Número de epochs\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model_2, loss_fn, optimizer)\n",
    "    test(test_dataloader, model_2, loss_fn)\n",
    "print(\"Fin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comprobamos que a pesar de aumentar drasticamente las capas los resultados no solo no mejoran sino que empeoran considerablemente. Parece que el aumento \"random\" de capas y neuronas ha provocado un problema de overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modificamos el dataloader aumentando el tamaño de batch a 100 e incluyendo una mezcla en la carga de datos de entrenamiento que se supone que ayuda a evitar el overfitting, además de definir el número de procesadores. Comprobamos que el resultado no ha variado, el modelo sigue ofreciendo resultados pesimos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_workers:  7\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.295583  [   32/60000]\n",
      "loss: 2.301068  [ 3232/60000]\n",
      "loss: 2.294047  [ 6432/60000]\n",
      "loss: 2.290294  [ 9632/60000]\n",
      "loss: 2.299824  [12832/60000]\n",
      "loss: 2.298966  [16032/60000]\n",
      "loss: 2.303476  [19232/60000]\n",
      "loss: 2.299732  [22432/60000]\n",
      "loss: 2.306796  [25632/60000]\n",
      "loss: 2.291769  [28832/60000]\n",
      "loss: 2.307497  [32032/60000]\n",
      "loss: 2.316036  [35232/60000]\n",
      "loss: 2.301544  [38432/60000]\n",
      "loss: 2.296035  [41632/60000]\n",
      "loss: 2.309145  [44832/60000]\n",
      "loss: 2.302687  [48032/60000]\n",
      "loss: 2.302456  [51232/60000]\n",
      "loss: 2.302650  [54432/60000]\n",
      "loss: 2.288232  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 11.3%, Avg loss: 2.298427 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.291005  [   32/60000]\n",
      "loss: 2.298913  [ 3232/60000]\n",
      "loss: 2.293610  [ 6432/60000]\n",
      "loss: 2.285895  [ 9632/60000]\n",
      "loss: 2.298414  [12832/60000]\n",
      "loss: 2.299239  [16032/60000]\n",
      "loss: 2.300195  [19232/60000]\n",
      "loss: 2.295631  [22432/60000]\n",
      "loss: 2.302909  [25632/60000]\n",
      "loss: 2.283360  [28832/60000]\n",
      "loss: 2.305176  [32032/60000]\n",
      "loss: 2.312315  [35232/60000]\n",
      "loss: 2.297594  [38432/60000]\n",
      "loss: 2.294337  [41632/60000]\n",
      "loss: 2.308789  [44832/60000]\n",
      "loss: 2.299408  [48032/60000]\n",
      "loss: 2.299401  [51232/60000]\n",
      "loss: 2.298064  [54432/60000]\n",
      "loss: 2.284728  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 11.3%, Avg loss: 2.294566 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.286401  [   32/60000]\n",
      "loss: 2.295988  [ 3232/60000]\n",
      "loss: 2.291269  [ 6432/60000]\n",
      "loss: 2.281397  [ 9632/60000]\n",
      "loss: 2.295891  [12832/60000]\n",
      "loss: 2.298148  [16032/60000]\n",
      "loss: 2.295472  [19232/60000]\n",
      "loss: 2.290390  [22432/60000]\n",
      "loss: 2.297940  [25632/60000]\n",
      "loss: 2.273956  [28832/60000]\n",
      "loss: 2.300962  [32032/60000]\n",
      "loss: 2.307233  [35232/60000]\n",
      "loss: 2.291938  [38432/60000]\n",
      "loss: 2.289863  [41632/60000]\n",
      "loss: 2.305985  [44832/60000]\n",
      "loss: 2.293936  [48032/60000]\n",
      "loss: 2.294005  [51232/60000]\n",
      "loss: 2.291037  [54432/60000]\n",
      "loss: 2.279682  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 11.3%, Avg loss: 2.288490 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.280487  [   32/60000]\n",
      "loss: 2.290990  [ 3232/60000]\n",
      "loss: 2.285598  [ 6432/60000]\n",
      "loss: 2.275517  [ 9632/60000]\n",
      "loss: 2.291139  [12832/60000]\n",
      "loss: 2.294637  [16032/60000]\n",
      "loss: 2.287856  [19232/60000]\n",
      "loss: 2.281978  [22432/60000]\n",
      "loss: 2.290071  [25632/60000]\n",
      "loss: 2.260701  [28832/60000]\n",
      "loss: 2.293363  [32032/60000]\n",
      "loss: 2.298998  [35232/60000]\n",
      "loss: 2.282611  [38432/60000]\n",
      "loss: 2.280170  [41632/60000]\n",
      "loss: 2.298998  [44832/60000]\n",
      "loss: 2.284329  [48032/60000]\n",
      "loss: 2.283962  [51232/60000]\n",
      "loss: 2.279182  [54432/60000]\n",
      "loss: 2.270908  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 15.1%, Avg loss: 2.277796 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2.270687  [   32/60000]\n",
      "loss: 2.281599  [ 3232/60000]\n",
      "loss: 2.274264  [ 6432/60000]\n",
      "loss: 2.265991  [ 9632/60000]\n",
      "loss: 2.282407  [12832/60000]\n",
      "loss: 2.287377  [16032/60000]\n",
      "loss: 2.273778  [19232/60000]\n",
      "loss: 2.266825  [22432/60000]\n",
      "loss: 2.276131  [25632/60000]\n",
      "loss: 2.237921  [28832/60000]\n",
      "loss: 2.279405  [32032/60000]\n",
      "loss: 2.284347  [35232/60000]\n",
      "loss: 2.265270  [38432/60000]\n",
      "loss: 2.260074  [41632/60000]\n",
      "loss: 2.284281  [44832/60000]\n",
      "loss: 2.265750  [48032/60000]\n",
      "loss: 2.264414  [51232/60000]\n",
      "loss: 2.257333  [54432/60000]\n",
      "loss: 2.253376  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 23.1%, Avg loss: 2.256720 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 2.251445  [   32/60000]\n",
      "loss: 2.263496  [ 3232/60000]\n",
      "loss: 2.250012  [ 6432/60000]\n",
      "loss: 2.247509  [ 9632/60000]\n",
      "loss: 2.264895  [12832/60000]\n",
      "loss: 2.271331  [16032/60000]\n",
      "loss: 2.243794  [19232/60000]\n",
      "loss: 2.236205  [22432/60000]\n",
      "loss: 2.247052  [25632/60000]\n",
      "loss: 2.190063  [28832/60000]\n",
      "loss: 2.249814  [32032/60000]\n",
      "loss: 2.254050  [35232/60000]\n",
      "loss: 2.227278  [38432/60000]\n",
      "loss: 2.213207  [41632/60000]\n",
      "loss: 2.250983  [44832/60000]\n",
      "loss: 2.223412  [48032/60000]\n",
      "loss: 2.219935  [51232/60000]\n",
      "loss: 2.207705  [54432/60000]\n",
      "loss: 2.212128  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 30.6%, Avg loss: 2.207404 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 2.206670  [   32/60000]\n",
      "loss: 2.221356  [ 3232/60000]\n",
      "loss: 2.190926  [ 6432/60000]\n",
      "loss: 2.204464  [ 9632/60000]\n",
      "loss: 2.224840  [12832/60000]\n",
      "loss: 2.232087  [16032/60000]\n",
      "loss: 2.166648  [19232/60000]\n",
      "loss: 2.158486  [22432/60000]\n",
      "loss: 2.174077  [25632/60000]\n",
      "loss: 2.064121  [28832/60000]\n",
      "loss: 2.174915  [32032/60000]\n",
      "loss: 2.175679  [35232/60000]\n",
      "loss: 2.125121  [38432/60000]\n",
      "loss: 2.078521  [41632/60000]\n",
      "loss: 2.161061  [44832/60000]\n",
      "loss: 2.109422  [48032/60000]\n",
      "loss: 2.094241  [51232/60000]\n",
      "loss: 2.069760  [54432/60000]\n",
      "loss: 2.096609  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 32.0%, Avg loss: 2.067686 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 2.079027  [   32/60000]\n",
      "loss: 2.102409  [ 3232/60000]\n",
      "loss: 2.027453  [ 6432/60000]\n",
      "loss: 2.082177  [ 9632/60000]\n",
      "loss: 2.115027  [12832/60000]\n",
      "loss: 2.120272  [16032/60000]\n",
      "loss: 1.948283  [19232/60000]\n",
      "loss: 1.935429  [22432/60000]\n",
      "loss: 1.964145  [25632/60000]\n",
      "loss: 1.722070  [28832/60000]\n",
      "loss: 1.955534  [32032/60000]\n",
      "loss: 1.946244  [35232/60000]\n",
      "loss: 1.858951  [38432/60000]\n",
      "loss: 1.748697  [41632/60000]\n",
      "loss: 1.905857  [44832/60000]\n",
      "loss: 1.808978  [48032/60000]\n",
      "loss: 1.764782  [51232/60000]\n",
      "loss: 1.716694  [54432/60000]\n",
      "loss: 1.761356  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 45.6%, Avg loss: 1.705774 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.728143  [   32/60000]\n",
      "loss: 1.781377  [ 3232/60000]\n",
      "loss: 1.655578  [ 6432/60000]\n",
      "loss: 1.670936  [ 9632/60000]\n",
      "loss: 1.734699  [12832/60000]\n",
      "loss: 1.757789  [16032/60000]\n",
      "loss: 1.447465  [19232/60000]\n",
      "loss: 1.424134  [22432/60000]\n",
      "loss: 1.437068  [25632/60000]\n",
      "loss: 1.149704  [28832/60000]\n",
      "loss: 1.348143  [32032/60000]\n",
      "loss: 1.384220  [35232/60000]\n",
      "loss: 1.389922  [38432/60000]\n",
      "loss: 1.253064  [41632/60000]\n",
      "loss: 1.352063  [44832/60000]\n",
      "loss: 1.194168  [48032/60000]\n",
      "loss: 1.167778  [51232/60000]\n",
      "loss: 1.065411  [54432/60000]\n",
      "loss: 1.121065  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Avg loss: 1.127001 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.195638  [   32/60000]\n",
      "loss: 1.282604  [ 3232/60000]\n",
      "loss: 1.188569  [ 6432/60000]\n",
      "loss: 1.069494  [ 9632/60000]\n",
      "loss: 1.175297  [12832/60000]\n",
      "loss: 1.283516  [16032/60000]\n",
      "loss: 0.930531  [19232/60000]\n",
      "loss: 0.894152  [22432/60000]\n",
      "loss: 0.960757  [25632/60000]\n",
      "loss: 0.713846  [28832/60000]\n",
      "loss: 0.810301  [32032/60000]\n",
      "loss: 0.909094  [35232/60000]\n",
      "loss: 1.012047  [38432/60000]\n",
      "loss: 0.857713  [41632/60000]\n",
      "loss: 0.965380  [44832/60000]\n",
      "loss: 0.864371  [48032/60000]\n",
      "loss: 0.815748  [51232/60000]\n",
      "loss: 0.699452  [54432/60000]\n",
      "loss: 0.815080  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 0.836694 \n",
      "\n",
      "Fin\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "class NeuralNetwork_3(nn.Module): # Clase que hereda de nn.Module y define la arquitectura de la red\n",
    "    def __init__(self): # Constructor de la clase\n",
    "        super().__init__() # Llama al constructor de la clase padre\n",
    "        self.flatten = nn.Flatten() # Capa de aplanamiento de la imagen (28x28 -> 784)\n",
    "        self.linear_relu_stack = nn.Sequential( # Secuencia de capas lineales y funciones de activación ReLU\n",
    "            nn.Linear(28*28, 512), # Capa de entrada con 784 entradas y 512 salidas\n",
    "            nn.ReLU(), # Función de activación ReLU después de la capa de entrada\n",
    "            nn.Linear(512, 64), # Capa oculta con 512 entradas y 64 salidas\n",
    "            nn.ReLU(), # Función de activación ReLU después de la capa oculta\n",
    "            nn.Linear(64, 128), # Capa oculta con 64 entradas y 128 salidas\n",
    "            nn.ReLU(), # Función de activación ReLU después de la capa oculta\n",
    "            nn.Linear(128, 512), # Capa oculta con 128 entradas y 512 salidas\n",
    "            nn.ReLU(), # Función de activación ReLU después de la capa oculta\n",
    "            nn.Linear(512, 10) # Capa de salida con 512 entradas y 10 salidas\n",
    "        )\n",
    "\n",
    "    def forward(self, x): # Método que define el flujo de datos a través de la red\n",
    "        x = self.flatten(x) # Aplana la imagen\n",
    "        logits = self.linear_relu_stack(x) # Pasa los datos a través de la secuencia de capas\n",
    "        return logits # Devuelve los logits (salida sin activación)\n",
    "    \n",
    "model_3 = NeuralNetwork_3() # Instancia del modelo\n",
    "\n",
    "num_workers = multiprocessing.cpu_count()-1\n",
    "print (\"num_workers: \", num_workers)\n",
    "train_dataloader = DataLoader(training_data, batch_size=32, shuffle=False, num_workers=num_workers)\n",
    "test_dataloader = DataLoader(test_data, batch_size=32, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.SGD( # Optimizador de descenso de gradiente estocástico\n",
    "    model_3.parameters(), # Parámetros del modelo a optimizar\n",
    "    lr=0.001 # Tasa de aprendizaje\n",
    "    )\n",
    "\n",
    "epochs = 10 # Número de epochs\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model_3, loss_fn, optimizer)\n",
    "    test(test_dataloader, model_3, loss_fn)\n",
    "print(\"Fin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vemos que la Exactitud no ha mejorado, por lo que optamos por realizar cambios en otros hiperparametros como el tamaño y número de capas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.302575  [   64/60000]\n",
      "loss: 2.287288  [ 6464/60000]\n",
      "loss: 2.274900  [12864/60000]\n",
      "loss: 2.235814  [19264/60000]\n",
      "loss: 2.210763  [25664/60000]\n",
      "loss: 2.147830  [32064/60000]\n",
      "loss: 1.985685  [38464/60000]\n",
      "loss: 1.857762  [44864/60000]\n",
      "loss: 1.366880  [51264/60000]\n",
      "loss: 1.032778  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.1%, Avg loss: 0.943124 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.034156  [   64/60000]\n",
      "loss: 0.753216  [ 6464/60000]\n",
      "loss: 0.720700  [12864/60000]\n",
      "loss: 0.607432  [19264/60000]\n",
      "loss: 0.575822  [25664/60000]\n",
      "loss: 0.470430  [32064/60000]\n",
      "loss: 0.398287  [38464/60000]\n",
      "loss: 0.531275  [44864/60000]\n",
      "loss: 0.527048  [51264/60000]\n",
      "loss: 0.514476  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.3%, Avg loss: 0.424852 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.486511  [   64/60000]\n",
      "loss: 0.327497  [ 6464/60000]\n",
      "loss: 0.355538  [12864/60000]\n",
      "loss: 0.406940  [19264/60000]\n",
      "loss: 0.361325  [25664/60000]\n",
      "loss: 0.368834  [32064/60000]\n",
      "loss: 0.241041  [38464/60000]\n",
      "loss: 0.413770  [44864/60000]\n",
      "loss: 0.425702  [51264/60000]\n",
      "loss: 0.448195  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.7%, Avg loss: 0.340464 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.311817  [   64/60000]\n",
      "loss: 0.261101  [ 6464/60000]\n",
      "loss: 0.248599  [12864/60000]\n",
      "loss: 0.357100  [19264/60000]\n",
      "loss: 0.287540  [25664/60000]\n",
      "loss: 0.318329  [32064/60000]\n",
      "loss: 0.190010  [38464/60000]\n",
      "loss: 0.364818  [44864/60000]\n",
      "loss: 0.355300  [51264/60000]\n",
      "loss: 0.404391  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 91.2%, Avg loss: 0.298483 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.236451  [   64/60000]\n",
      "loss: 0.238631  [ 6464/60000]\n",
      "loss: 0.194750  [12864/60000]\n",
      "loss: 0.335117  [19264/60000]\n",
      "loss: 0.238534  [25664/60000]\n",
      "loss: 0.287624  [32064/60000]\n",
      "loss: 0.163781  [38464/60000]\n",
      "loss: 0.332252  [44864/60000]\n",
      "loss: 0.295681  [51264/60000]\n",
      "loss: 0.363708  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.267612 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.195079  [   64/60000]\n",
      "loss: 0.222668  [ 6464/60000]\n",
      "loss: 0.162235  [12864/60000]\n",
      "loss: 0.319235  [19264/60000]\n",
      "loss: 0.202618  [25664/60000]\n",
      "loss: 0.268120  [32064/60000]\n",
      "loss: 0.146422  [38464/60000]\n",
      "loss: 0.308544  [44864/60000]\n",
      "loss: 0.248103  [51264/60000]\n",
      "loss: 0.330709  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 92.9%, Avg loss: 0.242633 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.167508  [   64/60000]\n",
      "loss: 0.209356  [ 6464/60000]\n",
      "loss: 0.140890  [12864/60000]\n",
      "loss: 0.303508  [19264/60000]\n",
      "loss: 0.174043  [25664/60000]\n",
      "loss: 0.255378  [32064/60000]\n",
      "loss: 0.134207  [38464/60000]\n",
      "loss: 0.289588  [44864/60000]\n",
      "loss: 0.213562  [51264/60000]\n",
      "loss: 0.305997  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 93.6%, Avg loss: 0.221379 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.146701  [   64/60000]\n",
      "loss: 0.197917  [ 6464/60000]\n",
      "loss: 0.124741  [12864/60000]\n",
      "loss: 0.288295  [19264/60000]\n",
      "loss: 0.151712  [25664/60000]\n",
      "loss: 0.245328  [32064/60000]\n",
      "loss: 0.124808  [38464/60000]\n",
      "loss: 0.271942  [44864/60000]\n",
      "loss: 0.188208  [51264/60000]\n",
      "loss: 0.287003  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 94.1%, Avg loss: 0.203005 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.129751  [   64/60000]\n",
      "loss: 0.188505  [ 6464/60000]\n",
      "loss: 0.112794  [12864/60000]\n",
      "loss: 0.271950  [19264/60000]\n",
      "loss: 0.134354  [25664/60000]\n",
      "loss: 0.236057  [32064/60000]\n",
      "loss: 0.117443  [38464/60000]\n",
      "loss: 0.255875  [44864/60000]\n",
      "loss: 0.169635  [51264/60000]\n",
      "loss: 0.271774  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 94.4%, Avg loss: 0.187019 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.115536  [   64/60000]\n",
      "loss: 0.180789  [ 6464/60000]\n",
      "loss: 0.103268  [12864/60000]\n",
      "loss: 0.255616  [19264/60000]\n",
      "loss: 0.121500  [25664/60000]\n",
      "loss: 0.226557  [32064/60000]\n",
      "loss: 0.110888  [38464/60000]\n",
      "loss: 0.241415  [44864/60000]\n",
      "loss: 0.157290  [51264/60000]\n",
      "loss: 0.259227  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 94.8%, Avg loss: 0.172998 \n",
      "\n",
      "Fin\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class NeuralNetwork_4(nn.Module): # Clase que hereda de nn.Module y define la arquitectura de la red\n",
    "    def __init__(self): # Constructor de la clase\n",
    "        super().__init__() # Llama al constructor de la clase padre\n",
    "        self.flatten = nn.Flatten() # Capa de aplanamiento de la imagen (28x28 -> 784)\n",
    "        self.linear_relu_stack = nn.Sequential( # Secuencia de capas lineales y funciones de activación ReLU\n",
    "            nn.Linear(28*28, 1024), # Capa de entrada con 784 entradas y 1024 salidas\n",
    "            nn.ReLU(), # Función de activación ReLU después de la capa de entrada\n",
    "            nn.Linear(1024, 1024), # Capa oculta totalmente conectada con 1024 entradas y 1024 salidas\n",
    "            nn.ReLU(), # Función de activación ReLU después de la capa oculta\n",
    "            nn.Linear(1024, 1024), # Capa oculta totalmente conectada con 1024 entradas y 1024 salidas\n",
    "            nn.ReLU(), # Función de activación ReLU después de la capa oculta\n",
    "            nn.Linear(1024, 10) # Capa de salida con 1024 entradas y 10 salidas\n",
    "        )\n",
    "\n",
    "    def forward(self, x): # Método que define el flujo de datos a través de la red\n",
    "        x = self.flatten(x) # Aplana la imagen\n",
    "        logits = self.linear_relu_stack(x) # Pasa los datos a través de la secuencia de capas\n",
    "        return logits # Devuelve los logits (salida sin activación)\n",
    "    \n",
    "model_4 = NeuralNetwork_4() # Instancia del modelo\n",
    "\n",
    "# num_workers = multiprocessing.cpu_count()-1\n",
    "# print (num_workers)\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() # Función de pérdida\n",
    "optimizer = torch.optim.SGD( # Optimizador de descenso de gradiente estocástico\n",
    "    model_4.parameters(), # Parámetros del modelo a optimizar\n",
    "    lr=0.01 # Tasa de aprendizaje\n",
    "    )\n",
    "\n",
    "epochs = 10 # Número de epochs\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model_4, loss_fn, optimizer)\n",
    "    test(test_dataloader, model_4, loss_fn)\n",
    "print(\"Fin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comprobamos que la Exactitud ha mejorado, por lo que buscamos alguna técnica adicional para ir mejorando los resultados. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probamos alguna técnica extraída del informe \"Techniques to Improve Our MNIST Accuracy Without Using CNNs\" publicado en el blog https://medium.com/@anderaquerretamontoro/99-46-accuracy-on-mnist-without-cnn-712042530420 como  por ejemplo utilizar un learnig rate variable aumentando los Epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.304788  [   64/60000]\n",
      "loss: 0.313561  [ 6464/60000]\n",
      "loss: 0.306146  [12864/60000]\n",
      "loss: 0.342958  [19264/60000]\n",
      "loss: 0.109508  [25664/60000]\n",
      "loss: 0.301690  [32064/60000]\n",
      "loss: 0.082986  [38464/60000]\n",
      "loss: 0.162539  [44864/60000]\n",
      "loss: 0.242330  [51264/60000]\n",
      "loss: 0.242473  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 95.0%, Avg loss: 0.173275 \n",
      "\n",
      "\tLR:  0.1\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.183583  [   64/60000]\n",
      "loss: 0.147915  [ 6464/60000]\n",
      "loss: 0.081132  [12864/60000]\n",
      "loss: 0.182753  [19264/60000]\n",
      "loss: 0.335545  [25664/60000]\n",
      "loss: 0.169885  [32064/60000]\n",
      "loss: 0.051537  [38464/60000]\n",
      "loss: 0.189772  [44864/60000]\n",
      "loss: 0.181064  [51264/60000]\n",
      "loss: 0.121322  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.2%, Avg loss: 0.134185 \n",
      "\n",
      "\tLR:  0.1\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.115540  [   64/60000]\n",
      "loss: 0.130552  [ 6464/60000]\n",
      "loss: 0.102846  [12864/60000]\n",
      "loss: 0.202242  [19264/60000]\n",
      "loss: 0.032586  [25664/60000]\n",
      "loss: 0.075472  [32064/60000]\n",
      "loss: 0.046913  [38464/60000]\n",
      "loss: 0.079138  [44864/60000]\n",
      "loss: 0.154251  [51264/60000]\n",
      "loss: 0.066254  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.2%, Avg loss: 0.108894 \n",
      "\n",
      "\tLR:  0.1\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.044999  [   64/60000]\n",
      "loss: 0.109337  [ 6464/60000]\n",
      "loss: 0.083595  [12864/60000]\n",
      "loss: 0.118048  [19264/60000]\n",
      "loss: 0.037061  [25664/60000]\n",
      "loss: 0.093393  [32064/60000]\n",
      "loss: 0.044455  [38464/60000]\n",
      "loss: 0.055521  [44864/60000]\n",
      "loss: 0.091544  [51264/60000]\n",
      "loss: 0.085599  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.9%, Avg loss: 0.129223 \n",
      "\n",
      "\tLR:  0.1\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.043740  [   64/60000]\n",
      "loss: 0.092607  [ 6464/60000]\n",
      "loss: 0.109156  [12864/60000]\n",
      "loss: 0.071763  [19264/60000]\n",
      "loss: 0.018033  [25664/60000]\n",
      "loss: 0.037638  [32064/60000]\n",
      "loss: 0.080555  [38464/60000]\n",
      "loss: 0.072776  [44864/60000]\n",
      "loss: 0.162049  [51264/60000]\n",
      "loss: 0.046321  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.4%, Avg loss: 0.168565 \n",
      "\n",
      "\tLR:  0.1\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.061040  [   64/60000]\n",
      "loss: 0.011709  [ 6464/60000]\n",
      "loss: 0.090589  [12864/60000]\n",
      "loss: 0.283247  [19264/60000]\n",
      "loss: 0.031465  [25664/60000]\n",
      "loss: 0.010136  [32064/60000]\n",
      "loss: 0.012444  [38464/60000]\n",
      "loss: 0.050654  [44864/60000]\n",
      "loss: 0.118611  [51264/60000]\n",
      "loss: 0.022397  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.6%, Avg loss: 0.090072 \n",
      "\n",
      "\tLR:  0.1\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.027691  [   64/60000]\n",
      "loss: 0.023293  [ 6464/60000]\n",
      "loss: 0.058578  [12864/60000]\n",
      "loss: 0.017854  [19264/60000]\n",
      "loss: 0.028781  [25664/60000]\n",
      "loss: 0.029884  [32064/60000]\n",
      "loss: 0.027661  [38464/60000]\n",
      "loss: 0.020661  [44864/60000]\n",
      "loss: 0.025300  [51264/60000]\n",
      "loss: 0.051790  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.6%, Avg loss: 0.100570 \n",
      "\n",
      "\tLR:  0.1\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.004599  [   64/60000]\n",
      "loss: 0.031301  [ 6464/60000]\n",
      "loss: 0.048197  [12864/60000]\n",
      "loss: 0.046440  [19264/60000]\n",
      "loss: 0.050867  [25664/60000]\n",
      "loss: 0.033956  [32064/60000]\n",
      "loss: 0.002613  [38464/60000]\n",
      "loss: 0.009725  [44864/60000]\n",
      "loss: 0.017480  [51264/60000]\n",
      "loss: 0.045638  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.7%, Avg loss: 0.097463 \n",
      "\n",
      "\tLR:  0.1\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.073035  [   64/60000]\n",
      "loss: 0.063850  [ 6464/60000]\n",
      "loss: 0.021206  [12864/60000]\n",
      "loss: 0.033578  [19264/60000]\n",
      "loss: 0.051474  [25664/60000]\n",
      "loss: 0.006466  [32064/60000]\n",
      "loss: 0.001528  [38464/60000]\n",
      "loss: 0.076894  [44864/60000]\n",
      "loss: 0.161253  [51264/60000]\n",
      "loss: 0.002768  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.0%, Avg loss: 0.082372 \n",
      "\n",
      "\tLR:  0.1\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.000856  [   64/60000]\n",
      "loss: 0.000350  [ 6464/60000]\n",
      "loss: 0.080749  [12864/60000]\n",
      "loss: 0.027078  [19264/60000]\n",
      "loss: 0.004308  [25664/60000]\n",
      "loss: 0.015807  [32064/60000]\n",
      "loss: 0.029246  [38464/60000]\n",
      "loss: 0.003634  [44864/60000]\n",
      "loss: 0.220922  [51264/60000]\n",
      "loss: 0.000519  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.7%, Avg loss: 0.098416 \n",
      "\n",
      "\tLR:  0.1\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.010151  [   64/60000]\n",
      "loss: 0.004715  [ 6464/60000]\n",
      "loss: 0.090227  [12864/60000]\n",
      "loss: 0.000844  [19264/60000]\n",
      "loss: 0.036058  [25664/60000]\n",
      "loss: 0.025601  [32064/60000]\n",
      "loss: 0.008144  [38464/60000]\n",
      "loss: 0.051142  [44864/60000]\n",
      "loss: 0.010639  [51264/60000]\n",
      "loss: 0.062870  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.089991 \n",
      "\n",
      "\tLR:  0.1\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.041970  [   64/60000]\n",
      "loss: 0.057613  [ 6464/60000]\n",
      "loss: 0.015964  [12864/60000]\n",
      "loss: 0.000978  [19264/60000]\n",
      "loss: 0.005408  [25664/60000]\n",
      "loss: 0.035568  [32064/60000]\n",
      "loss: 0.003106  [38464/60000]\n",
      "loss: 0.007012  [44864/60000]\n",
      "loss: 0.041503  [51264/60000]\n",
      "loss: 0.000106  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.2%, Avg loss: 0.086819 \n",
      "\n",
      "\tLR:  0.1\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.001820  [   64/60000]\n",
      "loss: 0.000830  [ 6464/60000]\n",
      "loss: 0.001987  [12864/60000]\n",
      "loss: 0.000481  [19264/60000]\n",
      "loss: 0.000421  [25664/60000]\n",
      "loss: 0.000544  [32064/60000]\n",
      "loss: 0.000640  [38464/60000]\n",
      "loss: 0.000449  [44864/60000]\n",
      "loss: 0.176873  [51264/60000]\n",
      "loss: 0.034147  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.4%, Avg loss: 0.125419 \n",
      "\n",
      "\tLR:  0.1\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.001939  [   64/60000]\n",
      "loss: 0.011772  [ 6464/60000]\n",
      "loss: 0.040357  [12864/60000]\n",
      "loss: 0.000381  [19264/60000]\n",
      "loss: 0.001779  [25664/60000]\n",
      "loss: 0.001230  [32064/60000]\n",
      "loss: 0.002401  [38464/60000]\n",
      "loss: 0.047158  [44864/60000]\n",
      "loss: 0.035653  [51264/60000]\n",
      "loss: 0.001439  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.8%, Avg loss: 0.113390 \n",
      "\n",
      "\tLR:  0.1\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.002920  [   64/60000]\n",
      "loss: 0.001099  [ 6464/60000]\n",
      "loss: 0.025392  [12864/60000]\n",
      "loss: 0.247321  [19264/60000]\n",
      "loss: 0.059675  [25664/60000]\n",
      "loss: 0.033310  [32064/60000]\n",
      "loss: 0.026238  [38464/60000]\n",
      "loss: 0.000668  [44864/60000]\n",
      "loss: 0.000059  [51264/60000]\n",
      "loss: 0.000132  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.2%, Avg loss: 0.101899 \n",
      "\n",
      "\tLR:  0.1\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.000291  [   64/60000]\n",
      "loss: 0.000068  [ 6464/60000]\n",
      "loss: 0.003332  [12864/60000]\n",
      "loss: 0.000177  [19264/60000]\n",
      "loss: 0.000844  [25664/60000]\n",
      "loss: 0.002590  [32064/60000]\n",
      "loss: 0.000068  [38464/60000]\n",
      "loss: 0.000217  [44864/60000]\n",
      "loss: 0.124351  [51264/60000]\n",
      "loss: 0.077899  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.4%, Avg loss: 0.171590 \n",
      "\n",
      "\tLR:  0.1\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.039430  [   64/60000]\n",
      "loss: 0.000852  [ 6464/60000]\n",
      "loss: 0.006342  [12864/60000]\n",
      "loss: 0.001133  [19264/60000]\n",
      "loss: 0.004065  [25664/60000]\n",
      "loss: 0.084934  [32064/60000]\n",
      "loss: 0.000408  [38464/60000]\n",
      "loss: 0.005919  [44864/60000]\n",
      "loss: 0.001512  [51264/60000]\n",
      "loss: 0.000128  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.086963 \n",
      "\n",
      "\tLR:  0.1\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.000301  [   64/60000]\n",
      "loss: 0.000610  [ 6464/60000]\n",
      "loss: 0.020322  [12864/60000]\n",
      "loss: 0.006821  [19264/60000]\n",
      "loss: 0.059568  [25664/60000]\n",
      "loss: 0.022930  [32064/60000]\n",
      "loss: 0.016220  [38464/60000]\n",
      "loss: 0.000297  [44864/60000]\n",
      "loss: 0.006766  [51264/60000]\n",
      "loss: 0.002898  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.086628 \n",
      "\n",
      "\tLR:  0.1\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.002173  [   64/60000]\n",
      "loss: 0.002690  [ 6464/60000]\n",
      "loss: 0.002555  [12864/60000]\n",
      "loss: 0.000251  [19264/60000]\n",
      "loss: 0.003433  [25664/60000]\n",
      "loss: 0.000678  [32064/60000]\n",
      "loss: 0.000803  [38464/60000]\n",
      "loss: 0.033357  [44864/60000]\n",
      "loss: 0.016103  [51264/60000]\n",
      "loss: 0.006105  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.088823 \n",
      "\n",
      "\tLR:  0.1\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.000338  [   64/60000]\n",
      "loss: 0.000095  [ 6464/60000]\n",
      "loss: 0.027017  [12864/60000]\n",
      "loss: 0.044368  [19264/60000]\n",
      "loss: 0.001488  [25664/60000]\n",
      "loss: 0.000495  [32064/60000]\n",
      "loss: 0.000940  [38464/60000]\n",
      "loss: 0.000074  [44864/60000]\n",
      "loss: 0.030936  [51264/60000]\n",
      "loss: 0.002167  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.096248 \n",
      "\n",
      "\tLR:  0.1\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.000284  [   64/60000]\n",
      "loss: 0.010468  [ 6464/60000]\n",
      "loss: 0.000901  [12864/60000]\n",
      "loss: 0.000093  [19264/60000]\n",
      "loss: 0.005011  [25664/60000]\n",
      "loss: 0.000032  [32064/60000]\n",
      "loss: 0.000144  [38464/60000]\n",
      "loss: 0.004921  [44864/60000]\n",
      "loss: 0.002235  [51264/60000]\n",
      "loss: 0.000085  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.092396 \n",
      "\n",
      "\tLR:  0.1\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.000414  [   64/60000]\n",
      "loss: 0.000013  [ 6464/60000]\n",
      "loss: 0.002297  [12864/60000]\n",
      "loss: 0.002301  [19264/60000]\n",
      "loss: 0.000006  [25664/60000]\n",
      "loss: 0.000264  [32064/60000]\n",
      "loss: 0.089238  [38464/60000]\n",
      "loss: 0.004230  [44864/60000]\n",
      "loss: 0.000133  [51264/60000]\n",
      "loss: 0.002015  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.086859 \n",
      "\n",
      "\tLR:  0.1\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.001280  [   64/60000]\n",
      "loss: 0.000222  [ 6464/60000]\n",
      "loss: 0.016539  [12864/60000]\n",
      "loss: 0.001283  [19264/60000]\n",
      "loss: 0.000009  [25664/60000]\n",
      "loss: 0.000094  [32064/60000]\n",
      "loss: 0.000326  [38464/60000]\n",
      "loss: 0.000953  [44864/60000]\n",
      "loss: 0.001605  [51264/60000]\n",
      "loss: 0.001361  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.9%, Avg loss: 0.117247 \n",
      "\n",
      "\tLR:  0.1\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.000952  [   64/60000]\n",
      "loss: 0.018650  [ 6464/60000]\n",
      "loss: 0.025729  [12864/60000]\n",
      "loss: 0.070939  [19264/60000]\n",
      "loss: 0.000323  [25664/60000]\n",
      "loss: 0.000472  [32064/60000]\n",
      "loss: 0.000067  [38464/60000]\n",
      "loss: 0.005308  [44864/60000]\n",
      "loss: 0.025095  [51264/60000]\n",
      "loss: 0.001505  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.8%, Avg loss: 0.123556 \n",
      "\n",
      "\tLR:  0.1\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.000422  [   64/60000]\n",
      "loss: 0.032438  [ 6464/60000]\n",
      "loss: 0.002889  [12864/60000]\n",
      "loss: 0.004694  [19264/60000]\n",
      "loss: 0.008256  [25664/60000]\n",
      "loss: 0.001340  [32064/60000]\n",
      "loss: 0.002594  [38464/60000]\n",
      "loss: 0.011838  [44864/60000]\n",
      "loss: 0.000299  [51264/60000]\n",
      "loss: 0.000018  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.2%, Avg loss: 0.098517 \n",
      "\n",
      "\tLR:  0.010000000000000002\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.005485  [   64/60000]\n",
      "loss: 0.000161  [ 6464/60000]\n",
      "loss: 0.001185  [12864/60000]\n",
      "loss: 0.000148  [19264/60000]\n",
      "loss: 0.000032  [25664/60000]\n",
      "loss: 0.000652  [32064/60000]\n",
      "loss: 0.000162  [38464/60000]\n",
      "loss: 0.000152  [44864/60000]\n",
      "loss: 0.000008  [51264/60000]\n",
      "loss: 0.000008  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.082743 \n",
      "\n",
      "\tLR:  0.010000000000000002\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.001106  [   64/60000]\n",
      "loss: 0.000014  [ 6464/60000]\n",
      "loss: 0.000345  [12864/60000]\n",
      "loss: 0.000106  [19264/60000]\n",
      "loss: 0.000016  [25664/60000]\n",
      "loss: 0.000431  [32064/60000]\n",
      "loss: 0.000149  [38464/60000]\n",
      "loss: 0.000136  [44864/60000]\n",
      "loss: 0.000008  [51264/60000]\n",
      "loss: 0.000008  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.082956 \n",
      "\n",
      "\tLR:  0.010000000000000002\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.000809  [   64/60000]\n",
      "loss: 0.000014  [ 6464/60000]\n",
      "loss: 0.000245  [12864/60000]\n",
      "loss: 0.000075  [19264/60000]\n",
      "loss: 0.000013  [25664/60000]\n",
      "loss: 0.000338  [32064/60000]\n",
      "loss: 0.000133  [38464/60000]\n",
      "loss: 0.000125  [44864/60000]\n",
      "loss: 0.000008  [51264/60000]\n",
      "loss: 0.000008  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.083450 \n",
      "\n",
      "\tLR:  0.010000000000000002\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.000664  [   64/60000]\n",
      "loss: 0.000014  [ 6464/60000]\n",
      "loss: 0.000198  [12864/60000]\n",
      "loss: 0.000058  [19264/60000]\n",
      "loss: 0.000012  [25664/60000]\n",
      "loss: 0.000291  [32064/60000]\n",
      "loss: 0.000121  [38464/60000]\n",
      "loss: 0.000115  [44864/60000]\n",
      "loss: 0.000007  [51264/60000]\n",
      "loss: 0.000008  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.083954 \n",
      "\n",
      "\tLR:  0.010000000000000002\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.000568  [   64/60000]\n",
      "loss: 0.000013  [ 6464/60000]\n",
      "loss: 0.000170  [12864/60000]\n",
      "loss: 0.000048  [19264/60000]\n",
      "loss: 0.000011  [25664/60000]\n",
      "loss: 0.000256  [32064/60000]\n",
      "loss: 0.000111  [38464/60000]\n",
      "loss: 0.000105  [44864/60000]\n",
      "loss: 0.000006  [51264/60000]\n",
      "loss: 0.000008  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.084441 \n",
      "\n",
      "\tLR:  0.010000000000000002\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.000501  [   64/60000]\n",
      "loss: 0.000012  [ 6464/60000]\n",
      "loss: 0.000150  [12864/60000]\n",
      "loss: 0.000040  [19264/60000]\n",
      "loss: 0.000010  [25664/60000]\n",
      "loss: 0.000227  [32064/60000]\n",
      "loss: 0.000103  [38464/60000]\n",
      "loss: 0.000097  [44864/60000]\n",
      "loss: 0.000005  [51264/60000]\n",
      "loss: 0.000007  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.084889 \n",
      "\n",
      "\tLR:  0.010000000000000002\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.000453  [   64/60000]\n",
      "loss: 0.000012  [ 6464/60000]\n",
      "loss: 0.000136  [12864/60000]\n",
      "loss: 0.000035  [19264/60000]\n",
      "loss: 0.000009  [25664/60000]\n",
      "loss: 0.000202  [32064/60000]\n",
      "loss: 0.000096  [38464/60000]\n",
      "loss: 0.000089  [44864/60000]\n",
      "loss: 0.000005  [51264/60000]\n",
      "loss: 0.000007  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.085306 \n",
      "\n",
      "\tLR:  0.010000000000000002\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.000414  [   64/60000]\n",
      "loss: 0.000011  [ 6464/60000]\n",
      "loss: 0.000125  [12864/60000]\n",
      "loss: 0.000031  [19264/60000]\n",
      "loss: 0.000008  [25664/60000]\n",
      "loss: 0.000182  [32064/60000]\n",
      "loss: 0.000090  [38464/60000]\n",
      "loss: 0.000082  [44864/60000]\n",
      "loss: 0.000005  [51264/60000]\n",
      "loss: 0.000007  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.085699 \n",
      "\n",
      "\tLR:  0.010000000000000002\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.000383  [   64/60000]\n",
      "loss: 0.000011  [ 6464/60000]\n",
      "loss: 0.000116  [12864/60000]\n",
      "loss: 0.000027  [19264/60000]\n",
      "loss: 0.000008  [25664/60000]\n",
      "loss: 0.000164  [32064/60000]\n",
      "loss: 0.000084  [38464/60000]\n",
      "loss: 0.000076  [44864/60000]\n",
      "loss: 0.000004  [51264/60000]\n",
      "loss: 0.000006  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.086063 \n",
      "\n",
      "\tLR:  0.010000000000000002\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.000357  [   64/60000]\n",
      "loss: 0.000010  [ 6464/60000]\n",
      "loss: 0.000108  [12864/60000]\n",
      "loss: 0.000025  [19264/60000]\n",
      "loss: 0.000007  [25664/60000]\n",
      "loss: 0.000150  [32064/60000]\n",
      "loss: 0.000080  [38464/60000]\n",
      "loss: 0.000071  [44864/60000]\n",
      "loss: 0.000004  [51264/60000]\n",
      "loss: 0.000006  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.086411 \n",
      "\n",
      "\tLR:  0.010000000000000002\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.000335  [   64/60000]\n",
      "loss: 0.000010  [ 6464/60000]\n",
      "loss: 0.000102  [12864/60000]\n",
      "loss: 0.000023  [19264/60000]\n",
      "loss: 0.000007  [25664/60000]\n",
      "loss: 0.000137  [32064/60000]\n",
      "loss: 0.000076  [38464/60000]\n",
      "loss: 0.000066  [44864/60000]\n",
      "loss: 0.000004  [51264/60000]\n",
      "loss: 0.000006  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.086740 \n",
      "\n",
      "\tLR:  0.010000000000000002\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.000316  [   64/60000]\n",
      "loss: 0.000009  [ 6464/60000]\n",
      "loss: 0.000097  [12864/60000]\n",
      "loss: 0.000021  [19264/60000]\n",
      "loss: 0.000006  [25664/60000]\n",
      "loss: 0.000126  [32064/60000]\n",
      "loss: 0.000072  [38464/60000]\n",
      "loss: 0.000062  [44864/60000]\n",
      "loss: 0.000003  [51264/60000]\n",
      "loss: 0.000006  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.087055 \n",
      "\n",
      "\tLR:  0.010000000000000002\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.000299  [   64/60000]\n",
      "loss: 0.000009  [ 6464/60000]\n",
      "loss: 0.000092  [12864/60000]\n",
      "loss: 0.000019  [19264/60000]\n",
      "loss: 0.000006  [25664/60000]\n",
      "loss: 0.000116  [32064/60000]\n",
      "loss: 0.000069  [38464/60000]\n",
      "loss: 0.000058  [44864/60000]\n",
      "loss: 0.000003  [51264/60000]\n",
      "loss: 0.000005  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.087352 \n",
      "\n",
      "\tLR:  0.010000000000000002\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.000284  [   64/60000]\n",
      "loss: 0.000009  [ 6464/60000]\n",
      "loss: 0.000088  [12864/60000]\n",
      "loss: 0.000018  [19264/60000]\n",
      "loss: 0.000006  [25664/60000]\n",
      "loss: 0.000107  [32064/60000]\n",
      "loss: 0.000066  [38464/60000]\n",
      "loss: 0.000054  [44864/60000]\n",
      "loss: 0.000003  [51264/60000]\n",
      "loss: 0.000005  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.087632 \n",
      "\n",
      "\tLR:  0.010000000000000002\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.000270  [   64/60000]\n",
      "loss: 0.000008  [ 6464/60000]\n",
      "loss: 0.000084  [12864/60000]\n",
      "loss: 0.000017  [19264/60000]\n",
      "loss: 0.000005  [25664/60000]\n",
      "loss: 0.000100  [32064/60000]\n",
      "loss: 0.000064  [38464/60000]\n",
      "loss: 0.000051  [44864/60000]\n",
      "loss: 0.000003  [51264/60000]\n",
      "loss: 0.000005  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.087898 \n",
      "\n",
      "\tLR:  0.010000000000000002\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.000258  [   64/60000]\n",
      "loss: 0.000008  [ 6464/60000]\n",
      "loss: 0.000081  [12864/60000]\n",
      "loss: 0.000016  [19264/60000]\n",
      "loss: 0.000005  [25664/60000]\n",
      "loss: 0.000093  [32064/60000]\n",
      "loss: 0.000062  [38464/60000]\n",
      "loss: 0.000049  [44864/60000]\n",
      "loss: 0.000003  [51264/60000]\n",
      "loss: 0.000005  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.088153 \n",
      "\n",
      "\tLR:  0.010000000000000002\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.000248  [   64/60000]\n",
      "loss: 0.000008  [ 6464/60000]\n",
      "loss: 0.000078  [12864/60000]\n",
      "loss: 0.000015  [19264/60000]\n",
      "loss: 0.000005  [25664/60000]\n",
      "loss: 0.000087  [32064/60000]\n",
      "loss: 0.000059  [38464/60000]\n",
      "loss: 0.000046  [44864/60000]\n",
      "loss: 0.000002  [51264/60000]\n",
      "loss: 0.000004  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.088400 \n",
      "\n",
      "\tLR:  0.010000000000000002\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.000238  [   64/60000]\n",
      "loss: 0.000008  [ 6464/60000]\n",
      "loss: 0.000075  [12864/60000]\n",
      "loss: 0.000014  [19264/60000]\n",
      "loss: 0.000005  [25664/60000]\n",
      "loss: 0.000082  [32064/60000]\n",
      "loss: 0.000057  [38464/60000]\n",
      "loss: 0.000044  [44864/60000]\n",
      "loss: 0.000002  [51264/60000]\n",
      "loss: 0.000004  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.088635 \n",
      "\n",
      "\tLR:  0.010000000000000002\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.000229  [   64/60000]\n",
      "loss: 0.000008  [ 6464/60000]\n",
      "loss: 0.000072  [12864/60000]\n",
      "loss: 0.000014  [19264/60000]\n",
      "loss: 0.000004  [25664/60000]\n",
      "loss: 0.000077  [32064/60000]\n",
      "loss: 0.000056  [38464/60000]\n",
      "loss: 0.000042  [44864/60000]\n",
      "loss: 0.000002  [51264/60000]\n",
      "loss: 0.000004  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.088864 \n",
      "\n",
      "\tLR:  0.010000000000000002\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.000220  [   64/60000]\n",
      "loss: 0.000007  [ 6464/60000]\n",
      "loss: 0.000070  [12864/60000]\n",
      "loss: 0.000013  [19264/60000]\n",
      "loss: 0.000004  [25664/60000]\n",
      "loss: 0.000073  [32064/60000]\n",
      "loss: 0.000054  [38464/60000]\n",
      "loss: 0.000040  [44864/60000]\n",
      "loss: 0.000002  [51264/60000]\n",
      "loss: 0.000004  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.089083 \n",
      "\n",
      "\tLR:  0.010000000000000002\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.000213  [   64/60000]\n",
      "loss: 0.000007  [ 6464/60000]\n",
      "loss: 0.000068  [12864/60000]\n",
      "loss: 0.000012  [19264/60000]\n",
      "loss: 0.000004  [25664/60000]\n",
      "loss: 0.000069  [32064/60000]\n",
      "loss: 0.000052  [38464/60000]\n",
      "loss: 0.000038  [44864/60000]\n",
      "loss: 0.000002  [51264/60000]\n",
      "loss: 0.000004  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.089300 \n",
      "\n",
      "\tLR:  0.010000000000000002\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.000206  [   64/60000]\n",
      "loss: 0.000007  [ 6464/60000]\n",
      "loss: 0.000066  [12864/60000]\n",
      "loss: 0.000012  [19264/60000]\n",
      "loss: 0.000004  [25664/60000]\n",
      "loss: 0.000065  [32064/60000]\n",
      "loss: 0.000051  [38464/60000]\n",
      "loss: 0.000037  [44864/60000]\n",
      "loss: 0.000002  [51264/60000]\n",
      "loss: 0.000004  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.089507 \n",
      "\n",
      "\tLR:  0.010000000000000002\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.000199  [   64/60000]\n",
      "loss: 0.000007  [ 6464/60000]\n",
      "loss: 0.000064  [12864/60000]\n",
      "loss: 0.000011  [19264/60000]\n",
      "loss: 0.000004  [25664/60000]\n",
      "loss: 0.000062  [32064/60000]\n",
      "loss: 0.000049  [38464/60000]\n",
      "loss: 0.000035  [44864/60000]\n",
      "loss: 0.000002  [51264/60000]\n",
      "loss: 0.000004  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.089708 \n",
      "\n",
      "\tLR:  0.010000000000000002\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.000193  [   64/60000]\n",
      "loss: 0.000007  [ 6464/60000]\n",
      "loss: 0.000062  [12864/60000]\n",
      "loss: 0.000011  [19264/60000]\n",
      "loss: 0.000004  [25664/60000]\n",
      "loss: 0.000059  [32064/60000]\n",
      "loss: 0.000048  [38464/60000]\n",
      "loss: 0.000034  [44864/60000]\n",
      "loss: 0.000002  [51264/60000]\n",
      "loss: 0.000003  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.089903 \n",
      "\n",
      "\tLR:  0.010000000000000002\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.000187  [   64/60000]\n",
      "loss: 0.000007  [ 6464/60000]\n",
      "loss: 0.000061  [12864/60000]\n",
      "loss: 0.000010  [19264/60000]\n",
      "loss: 0.000003  [25664/60000]\n",
      "loss: 0.000056  [32064/60000]\n",
      "loss: 0.000047  [38464/60000]\n",
      "loss: 0.000033  [44864/60000]\n",
      "loss: 0.000002  [51264/60000]\n",
      "loss: 0.000003  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.090088 \n",
      "\n",
      "\tLR:  0.0010000000000000002\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.000182  [   64/60000]\n",
      "loss: 0.000006  [ 6464/60000]\n",
      "loss: 0.000059  [12864/60000]\n",
      "loss: 0.000010  [19264/60000]\n",
      "loss: 0.000003  [25664/60000]\n",
      "loss: 0.000053  [32064/60000]\n",
      "loss: 0.000046  [38464/60000]\n",
      "loss: 0.000033  [44864/60000]\n",
      "loss: 0.000002  [51264/60000]\n",
      "loss: 0.000003  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.090106 \n",
      "\n",
      "\tLR:  0.0010000000000000002\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.000182  [   64/60000]\n",
      "loss: 0.000006  [ 6464/60000]\n",
      "loss: 0.000059  [12864/60000]\n",
      "loss: 0.000010  [19264/60000]\n",
      "loss: 0.000003  [25664/60000]\n",
      "loss: 0.000053  [32064/60000]\n",
      "loss: 0.000046  [38464/60000]\n",
      "loss: 0.000032  [44864/60000]\n",
      "loss: 0.000002  [51264/60000]\n",
      "loss: 0.000003  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.090124 \n",
      "\n",
      "\tLR:  0.0010000000000000002\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.000181  [   64/60000]\n",
      "loss: 0.000006  [ 6464/60000]\n",
      "loss: 0.000059  [12864/60000]\n",
      "loss: 0.000010  [19264/60000]\n",
      "loss: 0.000003  [25664/60000]\n",
      "loss: 0.000053  [32064/60000]\n",
      "loss: 0.000046  [38464/60000]\n",
      "loss: 0.000032  [44864/60000]\n",
      "loss: 0.000002  [51264/60000]\n",
      "loss: 0.000003  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.090142 \n",
      "\n",
      "\tLR:  0.0010000000000000002\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.000181  [   64/60000]\n",
      "loss: 0.000006  [ 6464/60000]\n",
      "loss: 0.000059  [12864/60000]\n",
      "loss: 0.000010  [19264/60000]\n",
      "loss: 0.000003  [25664/60000]\n",
      "loss: 0.000052  [32064/60000]\n",
      "loss: 0.000046  [38464/60000]\n",
      "loss: 0.000032  [44864/60000]\n",
      "loss: 0.000002  [51264/60000]\n",
      "loss: 0.000003  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.090160 \n",
      "\n",
      "\tLR:  0.0010000000000000002\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.000180  [   64/60000]\n",
      "loss: 0.000006  [ 6464/60000]\n",
      "loss: 0.000059  [12864/60000]\n",
      "loss: 0.000010  [19264/60000]\n",
      "loss: 0.000003  [25664/60000]\n",
      "loss: 0.000052  [32064/60000]\n",
      "loss: 0.000046  [38464/60000]\n",
      "loss: 0.000032  [44864/60000]\n",
      "loss: 0.000002  [51264/60000]\n",
      "loss: 0.000003  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.090178 \n",
      "\n",
      "\tLR:  0.0010000000000000002\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.000180  [   64/60000]\n",
      "loss: 0.000006  [ 6464/60000]\n",
      "loss: 0.000059  [12864/60000]\n",
      "loss: 0.000010  [19264/60000]\n",
      "loss: 0.000003  [25664/60000]\n",
      "loss: 0.000052  [32064/60000]\n",
      "loss: 0.000046  [38464/60000]\n",
      "loss: 0.000032  [44864/60000]\n",
      "loss: 0.000002  [51264/60000]\n",
      "loss: 0.000003  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.090195 \n",
      "\n",
      "\tLR:  0.0010000000000000002\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.000179  [   64/60000]\n",
      "loss: 0.000006  [ 6464/60000]\n",
      "loss: 0.000059  [12864/60000]\n",
      "loss: 0.000010  [19264/60000]\n",
      "loss: 0.000003  [25664/60000]\n",
      "loss: 0.000052  [32064/60000]\n",
      "loss: 0.000046  [38464/60000]\n",
      "loss: 0.000032  [44864/60000]\n",
      "loss: 0.000002  [51264/60000]\n",
      "loss: 0.000003  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.090213 \n",
      "\n",
      "\tLR:  0.0010000000000000002\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.000179  [   64/60000]\n",
      "loss: 0.000006  [ 6464/60000]\n",
      "loss: 0.000059  [12864/60000]\n",
      "loss: 0.000010  [19264/60000]\n",
      "loss: 0.000003  [25664/60000]\n",
      "loss: 0.000051  [32064/60000]\n",
      "loss: 0.000045  [38464/60000]\n",
      "loss: 0.000032  [44864/60000]\n",
      "loss: 0.000002  [51264/60000]\n",
      "loss: 0.000003  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.090231 \n",
      "\n",
      "\tLR:  0.0010000000000000002\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.000178  [   64/60000]\n",
      "loss: 0.000006  [ 6464/60000]\n",
      "loss: 0.000058  [12864/60000]\n",
      "loss: 0.000010  [19264/60000]\n",
      "loss: 0.000003  [25664/60000]\n",
      "loss: 0.000051  [32064/60000]\n",
      "loss: 0.000045  [38464/60000]\n",
      "loss: 0.000032  [44864/60000]\n",
      "loss: 0.000002  [51264/60000]\n",
      "loss: 0.000003  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.090248 \n",
      "\n",
      "\tLR:  0.0010000000000000002\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.000178  [   64/60000]\n",
      "loss: 0.000006  [ 6464/60000]\n",
      "loss: 0.000058  [12864/60000]\n",
      "loss: 0.000010  [19264/60000]\n",
      "loss: 0.000003  [25664/60000]\n",
      "loss: 0.000051  [32064/60000]\n",
      "loss: 0.000045  [38464/60000]\n",
      "loss: 0.000032  [44864/60000]\n",
      "loss: 0.000002  [51264/60000]\n",
      "loss: 0.000003  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.090266 \n",
      "\n",
      "\tLR:  0.0010000000000000002\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.000177  [   64/60000]\n",
      "loss: 0.000006  [ 6464/60000]\n",
      "loss: 0.000058  [12864/60000]\n",
      "loss: 0.000010  [19264/60000]\n",
      "loss: 0.000003  [25664/60000]\n",
      "loss: 0.000051  [32064/60000]\n",
      "loss: 0.000045  [38464/60000]\n",
      "loss: 0.000031  [44864/60000]\n",
      "loss: 0.000002  [51264/60000]\n",
      "loss: 0.000003  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.090284 \n",
      "\n",
      "\tLR:  0.0010000000000000002\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.000177  [   64/60000]\n",
      "loss: 0.000006  [ 6464/60000]\n",
      "loss: 0.000058  [12864/60000]\n",
      "loss: 0.000010  [19264/60000]\n",
      "loss: 0.000003  [25664/60000]\n",
      "loss: 0.000051  [32064/60000]\n",
      "loss: 0.000045  [38464/60000]\n",
      "loss: 0.000031  [44864/60000]\n",
      "loss: 0.000002  [51264/60000]\n",
      "loss: 0.000003  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.090301 \n",
      "\n",
      "\tLR:  0.0010000000000000002\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.000176  [   64/60000]\n",
      "loss: 0.000006  [ 6464/60000]\n",
      "loss: 0.000058  [12864/60000]\n",
      "loss: 0.000010  [19264/60000]\n",
      "loss: 0.000003  [25664/60000]\n",
      "loss: 0.000050  [32064/60000]\n",
      "loss: 0.000045  [38464/60000]\n",
      "loss: 0.000031  [44864/60000]\n",
      "loss: 0.000002  [51264/60000]\n",
      "loss: 0.000003  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.090318 \n",
      "\n",
      "\tLR:  0.0010000000000000002\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.000176  [   64/60000]\n",
      "loss: 0.000006  [ 6464/60000]\n",
      "loss: 0.000058  [12864/60000]\n",
      "loss: 0.000010  [19264/60000]\n",
      "loss: 0.000003  [25664/60000]\n",
      "loss: 0.000050  [32064/60000]\n",
      "loss: 0.000045  [38464/60000]\n",
      "loss: 0.000031  [44864/60000]\n",
      "loss: 0.000002  [51264/60000]\n",
      "loss: 0.000003  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.090336 \n",
      "\n",
      "\tLR:  0.0010000000000000002\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.000175  [   64/60000]\n",
      "loss: 0.000006  [ 6464/60000]\n",
      "loss: 0.000058  [12864/60000]\n",
      "loss: 0.000010  [19264/60000]\n",
      "loss: 0.000003  [25664/60000]\n",
      "loss: 0.000050  [32064/60000]\n",
      "loss: 0.000045  [38464/60000]\n",
      "loss: 0.000031  [44864/60000]\n",
      "loss: 0.000002  [51264/60000]\n",
      "loss: 0.000003  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.090353 \n",
      "\n",
      "\tLR:  0.0010000000000000002\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.000175  [   64/60000]\n",
      "loss: 0.000006  [ 6464/60000]\n",
      "loss: 0.000057  [12864/60000]\n",
      "loss: 0.000010  [19264/60000]\n",
      "loss: 0.000003  [25664/60000]\n",
      "loss: 0.000050  [32064/60000]\n",
      "loss: 0.000045  [38464/60000]\n",
      "loss: 0.000031  [44864/60000]\n",
      "loss: 0.000002  [51264/60000]\n",
      "loss: 0.000003  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.090370 \n",
      "\n",
      "\tLR:  0.0010000000000000002\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.000174  [   64/60000]\n",
      "loss: 0.000006  [ 6464/60000]\n",
      "loss: 0.000057  [12864/60000]\n",
      "loss: 0.000010  [19264/60000]\n",
      "loss: 0.000003  [25664/60000]\n",
      "loss: 0.000050  [32064/60000]\n",
      "loss: 0.000045  [38464/60000]\n",
      "loss: 0.000031  [44864/60000]\n",
      "loss: 0.000002  [51264/60000]\n",
      "loss: 0.000003  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.090387 \n",
      "\n",
      "\tLR:  0.0010000000000000002\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.000174  [   64/60000]\n",
      "loss: 0.000006  [ 6464/60000]\n",
      "loss: 0.000057  [12864/60000]\n",
      "loss: 0.000010  [19264/60000]\n",
      "loss: 0.000003  [25664/60000]\n",
      "loss: 0.000049  [32064/60000]\n",
      "loss: 0.000044  [38464/60000]\n",
      "loss: 0.000031  [44864/60000]\n",
      "loss: 0.000002  [51264/60000]\n",
      "loss: 0.000003  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.090405 \n",
      "\n",
      "\tLR:  0.0010000000000000002\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.000173  [   64/60000]\n",
      "loss: 0.000006  [ 6464/60000]\n",
      "loss: 0.000057  [12864/60000]\n",
      "loss: 0.000010  [19264/60000]\n",
      "loss: 0.000003  [25664/60000]\n",
      "loss: 0.000049  [32064/60000]\n",
      "loss: 0.000044  [38464/60000]\n",
      "loss: 0.000031  [44864/60000]\n",
      "loss: 0.000002  [51264/60000]\n",
      "loss: 0.000003  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.090422 \n",
      "\n",
      "\tLR:  0.0010000000000000002\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.000173  [   64/60000]\n",
      "loss: 0.000006  [ 6464/60000]\n",
      "loss: 0.000057  [12864/60000]\n",
      "loss: 0.000010  [19264/60000]\n",
      "loss: 0.000003  [25664/60000]\n",
      "loss: 0.000049  [32064/60000]\n",
      "loss: 0.000044  [38464/60000]\n",
      "loss: 0.000031  [44864/60000]\n",
      "loss: 0.000002  [51264/60000]\n",
      "loss: 0.000003  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.090439 \n",
      "\n",
      "\tLR:  0.0010000000000000002\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.000172  [   64/60000]\n",
      "loss: 0.000006  [ 6464/60000]\n",
      "loss: 0.000057  [12864/60000]\n",
      "loss: 0.000010  [19264/60000]\n",
      "loss: 0.000003  [25664/60000]\n",
      "loss: 0.000049  [32064/60000]\n",
      "loss: 0.000044  [38464/60000]\n",
      "loss: 0.000030  [44864/60000]\n",
      "loss: 0.000002  [51264/60000]\n",
      "loss: 0.000003  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.090455 \n",
      "\n",
      "\tLR:  0.0010000000000000002\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.000172  [   64/60000]\n",
      "loss: 0.000006  [ 6464/60000]\n",
      "loss: 0.000057  [12864/60000]\n",
      "loss: 0.000010  [19264/60000]\n",
      "loss: 0.000003  [25664/60000]\n",
      "loss: 0.000049  [32064/60000]\n",
      "loss: 0.000044  [38464/60000]\n",
      "loss: 0.000030  [44864/60000]\n",
      "loss: 0.000002  [51264/60000]\n",
      "loss: 0.000003  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.090472 \n",
      "\n",
      "\tLR:  0.0010000000000000002\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.000171  [   64/60000]\n",
      "loss: 0.000006  [ 6464/60000]\n",
      "loss: 0.000057  [12864/60000]\n",
      "loss: 0.000010  [19264/60000]\n",
      "loss: 0.000003  [25664/60000]\n",
      "loss: 0.000048  [32064/60000]\n",
      "loss: 0.000044  [38464/60000]\n",
      "loss: 0.000030  [44864/60000]\n",
      "loss: 0.000002  [51264/60000]\n",
      "loss: 0.000003  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.090489 \n",
      "\n",
      "\tLR:  0.0010000000000000002\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.000171  [   64/60000]\n",
      "loss: 0.000006  [ 6464/60000]\n",
      "loss: 0.000056  [12864/60000]\n",
      "loss: 0.000010  [19264/60000]\n",
      "loss: 0.000003  [25664/60000]\n",
      "loss: 0.000048  [32064/60000]\n",
      "loss: 0.000044  [38464/60000]\n",
      "loss: 0.000030  [44864/60000]\n",
      "loss: 0.000002  [51264/60000]\n",
      "loss: 0.000003  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.090506 \n",
      "\n",
      "\tLR:  0.0010000000000000002\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.000171  [   64/60000]\n",
      "loss: 0.000006  [ 6464/60000]\n",
      "loss: 0.000056  [12864/60000]\n",
      "loss: 0.000009  [19264/60000]\n",
      "loss: 0.000003  [25664/60000]\n",
      "loss: 0.000048  [32064/60000]\n",
      "loss: 0.000044  [38464/60000]\n",
      "loss: 0.000030  [44864/60000]\n",
      "loss: 0.000002  [51264/60000]\n",
      "loss: 0.000003  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.090523 \n",
      "\n",
      "\tLR:  0.0010000000000000002\n",
      "Fin\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class NeuralNetwork_5(nn.Module): # Clase que hereda de nn.Module y define la arquitectura de la red\n",
    "    def __init__(self): # Constructor de la clase\n",
    "        super().__init__() # Llama al constructor de la clase padre\n",
    "        self.flatten = nn.Flatten() # Capa de aplanamiento de la imagen (28x28 -> 784)\n",
    "        self.linear_relu_stack = nn.Sequential( # Secuencia de capas lineales y funciones de activación ReLU\n",
    "            nn.Linear(28*28, 1024), # Capa de entrada con 784 entradas y 1024 salidas\n",
    "            nn.ReLU(), # Función de activación ReLU después de la capa de entrada\n",
    "            nn.Linear(1024, 1024), # Capa oculta totalmente conectada con 1024 entradas y 1024 salidas\n",
    "            nn.ReLU(), # Función de activación ReLU después de la capa oculta\n",
    "            nn.Linear(1024, 1024), # Capa oculta totalmente conectada con 1024 entradas y 1024 salidas\n",
    "            nn.ReLU(), # Función de activación ReLU después de la capa oculta\n",
    "            nn.Linear(1024, 10) # Capa de salida con 1024 entradas y 10 salidas\n",
    "        )\n",
    "\n",
    "    def forward(self, x): # Método que define el flujo de datos a través de la red\n",
    "        x = self.flatten(x) # Aplana la imagen\n",
    "        logits = self.linear_relu_stack(x) # Pasa los datos a través de la secuencia de capas\n",
    "        return logits # Devuelve los logits (salida sin activación)\n",
    "    \n",
    "model_5 = NeuralNetwork_5() # Instancia del modelo\n",
    "optimizer = torch.optim.SGD(model_5.parameters(), lr=0.1, weight_decay=1e-6, momentum=0.9)\n",
    "\n",
    "# Learning Rate Annealing (LRA) scheduling\n",
    "# lr = 0.1     if epoch < 25\n",
    "# lr = 0.01    if 25 <= epoch < 50\n",
    "# lr = 0.001   if epoch >= 50\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[25, 50], gamma=0.1)\n",
    "\n",
    "\n",
    "# Start training\n",
    "epochs = 75 # Número de epochs\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model_5, loss_fn, optimizer)\n",
    "    test(test_dataloader, model_5, loss_fn)\n",
    "    scheduler.step()\n",
    "    print(\"\\tLearningRate: \", optimizer.param_groups[0]['lr'])\n",
    "print(\"Fin\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comprobamos que la Precisión ha subido hasta el 98.6%. Aunque posiblemente el hecho de aumentar los Epochs y modificar el learning rate también hubiese mejorado los resultados en el modelo original, el hecho de ir optimizando el número de capas, y que el learning rate sea dinámico durante el entrenamiento posiblemente ha hecho que el modelo consiga una fiabilidad alta. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:cornflowerblue\"><u>2. Crea otro modelo convolucional como el del ejemplo propuesto intentando mejorar la exactitud.</u> </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crearemos un modelo convolucional sobre el dataset FashionMnist intentando optimizar los hiperparámetros para mejorar sus valores de exactitud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Importamos las librerías necesarias\n",
    "\n",
    "Se importan las librerias de PyTorch necesarias para construir la Red Neuronal Convolucional. \n",
    "\n",
    "Se descarga el dataset FashionMnist separando el grupo de entrenamiento y test y transformando las imagenes a un Tensor de PyTorch.\n",
    "\n",
    "Cargamos el dataset en el DataLoader con un tamaño de lote o Batch de 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset FashionMNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "           ) \n",
      "\n",
      " torch.Size([60000, 28, 28]) \n",
      "\n",
      " \n",
      "\n",
      " tensor([9, 0, 0,  ..., 3, 0, 5])\n",
      "Shape de X [N(numero de muestras), C(canales de color), H(altura), W(anchura)]: torch.Size([64, 1, 28, 28])\n",
      "Shape de y: torch.Size([64]) torch.int64\n",
      "Shape de X [N(numero de muestras), C(canales de color), H(altura), W(anchura)]: torch.Size([64, 1, 28, 28])\n",
      "Shape de y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Definimos las transformaciones para preprocesado de las imágenes\n",
    "transform = transforms.Compose([transforms.ToTensor(),])\n",
    "\n",
    "# Cargamos el dataset Fashion MNIST\n",
    "train_Fdata = datasets.FashionMNIST('./data', train=True, download=True, transform=transform)\n",
    "test_Fdata = datasets.FashionMNIST('./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Creamos los dataloaders\n",
    "train_Floader = DataLoader(train_Fdata, batch_size=64, shuffle=True)\n",
    "test_Floader = DataLoader(test_Fdata, batch_size=64, shuffle=False)\n",
    "\n",
    "print (train_Fdata,\"\\n\\n\", train_Fdata.data.shape, \"\\n\\n\", \"\\n\\n\", train_Fdata.targets)\n",
    "\n",
    "for X, y in test_Floader:\n",
    "    print(f\"Shape de X [N(numero de muestras), C(canales de color), H(altura), W(anchura)]: {X.shape}\")\n",
    "    print(f\"Shape de y: {y.shape} {y.dtype}\")\n",
    "    break\n",
    "\n",
    "for X, y in train_Floader:\n",
    "    print(f\"Shape de X [N(numero de muestras), C(canales de color), H(altura), W(anchura)]: {X.shape}\")\n",
    "    print(f\"Shape de y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Definimos la red \n",
    "\n",
    "Definimos la clase a la que pertenecerá nuestro modelo mediante los métodos \"constructor (init)\" y \"forward\"y de la clase nn.Module, detallando los parámetros que vamos a incluir en nuestro modelo de Red Neuronal Convolucional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CNN(nn.Module): # Definimos la red neuronal convolucional\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1) # primera capa convolucional, 1 canal de color, 32 features, ventana de 3x3. Tamaño 32*28*28\n",
    "    self.pool = nn.MaxPool2d(2, 2) # capa de maxpooling, reduce el tamaño de la imagen a la mitad manteniendo las features\n",
    "    self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1) # segunda capa convolucional, 32 features de entrada, 64 de salida, ventana de 3x3. Tamaño de la imagen 64*14*14\n",
    "    self.fc1 = nn.Linear(7 * 7 * 64, 128) # capa linear de entrada, toma un tensor aplanado de 7*7*64 en la entrada y devuelve 128 en la salida\n",
    "    self.fc2 = nn.Linear(128, 10) # capa linear de salida, toma un tensor de 128 en la entrada y devuelve 10 en la salida que son el número de etiquetas del dataset\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.pool(F.relu(self.conv1(x))) # paso por 1ª capa convolucional del batch, aplica función de activación Relu y reduce tamaño con maxpooling (1,28,28->32,14,14)\n",
    "    x = self.pool(F.relu(self.conv2(x))) # paso por 2ª capa convolucional la salida de la 1ª capa, aplica función de activación Relu y reduce tamaño con maxpooling (32,14,14->64,7,7)\n",
    "    x = x.view(-1, 7 * 7 * 64) # Aplanamiento tras las capas convolucionales (flatten)\n",
    "    x = F.relu(self.fc1(x)) # paso por 1ª capa linear de la red neuronal, 3136 neuronas de entrada y 128 de salida, aplica funcion de activación Relu (3136,128)\n",
    "    x = F.log_softmax(self.fc2(x), dim=1)  # paso por 2ª capa de la red neuronal, 128 neuronas de entrada y 10 de salida, correspondientes con las etiquetas del dataset.\n",
    "    return x\n",
    "  \n",
    "modelConv = CNN() # Instanciamos la red neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.- Optimización y entrenamiento\n",
    "\n",
    "### Definimos el Optimizador y la Función de Pérdida para nuestro modelo. \n",
    "\n",
    "Definimos la función de perdida (CrossEntropyLoss) y el optimizador, en este caso Adam, que es una variante del descenso de gradiente estocástico que calcula tasas de aprendizaje individuales para diferentes parámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossFn = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam(modelConv.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definimos el método Train para entrenar nuestro modelo\n",
    "\n",
    "Para cada lote el método itera sobre los datos del lote o batch, establece una predicción, calcula la pérdida y activa el backpropagation reseteando los gradientes, calculando el nuevo gradiente de la función de pérdida y actualizando los parametros de peso y coste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/938], Loss: 0.3038\n",
      "Epoch [1/10], Step [200/938], Loss: 0.3650\n",
      "Epoch [1/10], Step [300/938], Loss: 0.5338\n",
      "Epoch [1/10], Step [400/938], Loss: 0.2693\n",
      "Epoch [1/10], Step [500/938], Loss: 0.3609\n",
      "Epoch [1/10], Step [600/938], Loss: 0.4540\n",
      "Epoch [1/10], Step [700/938], Loss: 0.3548\n",
      "Epoch [1/10], Step [800/938], Loss: 0.4457\n",
      "Epoch [1/10], Step [900/938], Loss: 0.3956\n",
      "Epoch [2/10], Step [100/938], Loss: 0.2378\n",
      "Epoch [2/10], Step [200/938], Loss: 0.3073\n",
      "Epoch [2/10], Step [300/938], Loss: 0.3441\n",
      "Epoch [2/10], Step [400/938], Loss: 0.1632\n",
      "Epoch [2/10], Step [500/938], Loss: 0.2529\n",
      "Epoch [2/10], Step [600/938], Loss: 0.3449\n",
      "Epoch [2/10], Step [700/938], Loss: 0.2809\n",
      "Epoch [2/10], Step [800/938], Loss: 0.2512\n",
      "Epoch [2/10], Step [900/938], Loss: 0.2544\n",
      "Epoch [3/10], Step [100/938], Loss: 0.3556\n",
      "Epoch [3/10], Step [200/938], Loss: 0.2067\n",
      "Epoch [3/10], Step [300/938], Loss: 0.3696\n",
      "Epoch [3/10], Step [400/938], Loss: 0.3144\n",
      "Epoch [3/10], Step [500/938], Loss: 0.2064\n",
      "Epoch [3/10], Step [600/938], Loss: 0.1406\n",
      "Epoch [3/10], Step [700/938], Loss: 0.2691\n",
      "Epoch [3/10], Step [800/938], Loss: 0.1672\n",
      "Epoch [3/10], Step [900/938], Loss: 0.2773\n",
      "Epoch [4/10], Step [100/938], Loss: 0.2285\n",
      "Epoch [4/10], Step [200/938], Loss: 0.2391\n",
      "Epoch [4/10], Step [300/938], Loss: 0.1826\n",
      "Epoch [4/10], Step [400/938], Loss: 0.2307\n",
      "Epoch [4/10], Step [500/938], Loss: 0.2318\n",
      "Epoch [4/10], Step [600/938], Loss: 0.2790\n",
      "Epoch [4/10], Step [700/938], Loss: 0.2961\n",
      "Epoch [4/10], Step [800/938], Loss: 0.1813\n",
      "Epoch [4/10], Step [900/938], Loss: 0.1098\n",
      "Epoch [5/10], Step [100/938], Loss: 0.2567\n",
      "Epoch [5/10], Step [200/938], Loss: 0.1848\n",
      "Epoch [5/10], Step [300/938], Loss: 0.2124\n",
      "Epoch [5/10], Step [400/938], Loss: 0.1631\n",
      "Epoch [5/10], Step [500/938], Loss: 0.0804\n",
      "Epoch [5/10], Step [600/938], Loss: 0.1548\n",
      "Epoch [5/10], Step [700/938], Loss: 0.2132\n",
      "Epoch [5/10], Step [800/938], Loss: 0.2650\n",
      "Epoch [5/10], Step [900/938], Loss: 0.1573\n"
     ]
    }
   ],
   "source": [
    "modelConv.train() # Ponemos el modelo en modo entrenamiento \n",
    "\n",
    "for epoch in range(5): # Definimos 10 epochs\n",
    "  \n",
    "  for i, (images, labels) in enumerate(train_Floader):\n",
    "    # Forward pass\n",
    "    outputs = modelConv(images) # Genera predicciones\n",
    "    loss = lossFn(outputs, labels) # Calcula la perdida en la iteración\n",
    "\n",
    "    # Backpropagation\n",
    "    optim.zero_grad() # Resetea los gradientes\n",
    "    loss.backward() # Calcula el gradiente de la función de pérdida\n",
    "    optim.step() # Actualiza los parámetros de pesos y sesgos\n",
    "\n",
    "    if (i + 1) % 100 == 0:\n",
    "      print(f'Epoch [{epoch+1}/{5}], Step [{i+1}/{len(train_Floader)}], Loss: {loss.item():.4f}')\n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Evaluación del modelo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 91.27%\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(): # Deshabilita el calculo de gradientes\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  for images, labels in test_Floader:\n",
    "    outputs = modelConv(images)\n",
    "    _, pred = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (pred == labels).sum().item()\n",
    "  print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comprobamos que la exactitud en este dataset es más baja, intentamos modificar los hiperparámetros para lograr un mejor valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/12], Step [100/938], Loss: 0.9874\n",
      "Epoch [1/12], Step [200/938], Loss: 0.6368\n",
      "Epoch [1/12], Step [300/938], Loss: 0.6046\n",
      "Epoch [1/12], Step [400/938], Loss: 0.6895\n",
      "Epoch [1/12], Step [500/938], Loss: 0.3520\n",
      "Epoch [1/12], Step [600/938], Loss: 0.4296\n",
      "Epoch [1/12], Step [700/938], Loss: 0.3135\n",
      "Epoch [1/12], Step [800/938], Loss: 0.3433\n",
      "Epoch [1/12], Step [900/938], Loss: 0.2742\n",
      "\tLearningRate:  0.1\n",
      "Epoch [2/12], Step [100/938], Loss: 0.3056\n",
      "Epoch [2/12], Step [200/938], Loss: 0.4752\n",
      "Epoch [2/12], Step [300/938], Loss: 0.3638\n",
      "Epoch [2/12], Step [400/938], Loss: 0.2030\n",
      "Epoch [2/12], Step [500/938], Loss: 0.2879\n",
      "Epoch [2/12], Step [600/938], Loss: 0.1926\n",
      "Epoch [2/12], Step [700/938], Loss: 0.2124\n",
      "Epoch [2/12], Step [800/938], Loss: 0.2833\n",
      "Epoch [2/12], Step [900/938], Loss: 0.2016\n",
      "\tLearningRate:  0.1\n",
      "Epoch [3/12], Step [100/938], Loss: 0.2480\n",
      "Epoch [3/12], Step [200/938], Loss: 0.2326\n",
      "Epoch [3/12], Step [300/938], Loss: 0.2321\n",
      "Epoch [3/12], Step [400/938], Loss: 0.2527\n",
      "Epoch [3/12], Step [500/938], Loss: 0.3721\n",
      "Epoch [3/12], Step [600/938], Loss: 0.4107\n",
      "Epoch [3/12], Step [700/938], Loss: 0.1730\n",
      "Epoch [3/12], Step [800/938], Loss: 0.2042\n",
      "Epoch [3/12], Step [900/938], Loss: 0.1403\n",
      "\tLearningRate:  0.1\n",
      "Epoch [4/12], Step [100/938], Loss: 0.1327\n",
      "Epoch [4/12], Step [200/938], Loss: 0.2033\n",
      "Epoch [4/12], Step [300/938], Loss: 0.3493\n",
      "Epoch [4/12], Step [400/938], Loss: 0.2117\n",
      "Epoch [4/12], Step [500/938], Loss: 0.2418\n",
      "Epoch [4/12], Step [600/938], Loss: 0.3301\n",
      "Epoch [4/12], Step [700/938], Loss: 0.2455\n",
      "Epoch [4/12], Step [800/938], Loss: 0.3476\n",
      "Epoch [4/12], Step [900/938], Loss: 0.1750\n",
      "\tLearningRate:  0.010000000000000002\n",
      "Epoch [5/12], Step [100/938], Loss: 0.0670\n",
      "Epoch [5/12], Step [200/938], Loss: 0.1602\n",
      "Epoch [5/12], Step [300/938], Loss: 0.1322\n",
      "Epoch [5/12], Step [400/938], Loss: 0.1221\n",
      "Epoch [5/12], Step [500/938], Loss: 0.2787\n",
      "Epoch [5/12], Step [600/938], Loss: 0.2129\n",
      "Epoch [5/12], Step [700/938], Loss: 0.1577\n",
      "Epoch [5/12], Step [800/938], Loss: 0.1119\n",
      "Epoch [5/12], Step [900/938], Loss: 0.1099\n",
      "\tLearningRate:  0.010000000000000002\n",
      "Epoch [6/12], Step [100/938], Loss: 0.0853\n",
      "Epoch [6/12], Step [200/938], Loss: 0.1359\n",
      "Epoch [6/12], Step [300/938], Loss: 0.2018\n",
      "Epoch [6/12], Step [400/938], Loss: 0.1140\n",
      "Epoch [6/12], Step [500/938], Loss: 0.0988\n",
      "Epoch [6/12], Step [600/938], Loss: 0.1449\n",
      "Epoch [6/12], Step [700/938], Loss: 0.0961\n",
      "Epoch [6/12], Step [800/938], Loss: 0.0851\n",
      "Epoch [6/12], Step [900/938], Loss: 0.1073\n",
      "\tLearningRate:  0.010000000000000002\n",
      "Epoch [7/12], Step [100/938], Loss: 0.0984\n",
      "Epoch [7/12], Step [200/938], Loss: 0.0949\n",
      "Epoch [7/12], Step [300/938], Loss: 0.0634\n",
      "Epoch [7/12], Step [400/938], Loss: 0.1019\n",
      "Epoch [7/12], Step [500/938], Loss: 0.1735\n",
      "Epoch [7/12], Step [600/938], Loss: 0.2197\n",
      "Epoch [7/12], Step [700/938], Loss: 0.1503\n",
      "Epoch [7/12], Step [800/938], Loss: 0.0916\n",
      "Epoch [7/12], Step [900/938], Loss: 0.0442\n",
      "\tLearningRate:  0.010000000000000002\n",
      "Epoch [8/12], Step [100/938], Loss: 0.1732\n",
      "Epoch [8/12], Step [200/938], Loss: 0.0415\n",
      "Epoch [8/12], Step [300/938], Loss: 0.1024\n",
      "Epoch [8/12], Step [400/938], Loss: 0.1012\n",
      "Epoch [8/12], Step [500/938], Loss: 0.1304\n",
      "Epoch [8/12], Step [600/938], Loss: 0.1298\n",
      "Epoch [8/12], Step [700/938], Loss: 0.1082\n",
      "Epoch [8/12], Step [800/938], Loss: 0.0754\n",
      "Epoch [8/12], Step [900/938], Loss: 0.1977\n",
      "\tLearningRate:  0.0010000000000000002\n",
      "Epoch [9/12], Step [100/938], Loss: 0.0656\n",
      "Epoch [9/12], Step [200/938], Loss: 0.1672\n",
      "Epoch [9/12], Step [300/938], Loss: 0.1205\n",
      "Epoch [9/12], Step [400/938], Loss: 0.0492\n",
      "Epoch [9/12], Step [500/938], Loss: 0.0383\n",
      "Epoch [9/12], Step [600/938], Loss: 0.1266\n",
      "Epoch [9/12], Step [700/938], Loss: 0.0433\n",
      "Epoch [9/12], Step [800/938], Loss: 0.0936\n",
      "Epoch [9/12], Step [900/938], Loss: 0.1485\n",
      "\tLearningRate:  0.0010000000000000002\n",
      "Epoch [10/12], Step [100/938], Loss: 0.0375\n",
      "Epoch [10/12], Step [200/938], Loss: 0.2162\n",
      "Epoch [10/12], Step [300/938], Loss: 0.1083\n",
      "Epoch [10/12], Step [400/938], Loss: 0.1035\n",
      "Epoch [10/12], Step [500/938], Loss: 0.1493\n",
      "Epoch [10/12], Step [600/938], Loss: 0.1179\n",
      "Epoch [10/12], Step [700/938], Loss: 0.1285\n",
      "Epoch [10/12], Step [800/938], Loss: 0.1278\n",
      "Epoch [10/12], Step [900/938], Loss: 0.0993\n",
      "\tLearningRate:  0.0010000000000000002\n",
      "Epoch [11/12], Step [100/938], Loss: 0.1716\n",
      "Epoch [11/12], Step [200/938], Loss: 0.1718\n",
      "Epoch [11/12], Step [300/938], Loss: 0.1088\n",
      "Epoch [11/12], Step [400/938], Loss: 0.0714\n",
      "Epoch [11/12], Step [500/938], Loss: 0.1288\n",
      "Epoch [11/12], Step [600/938], Loss: 0.0915\n",
      "Epoch [11/12], Step [700/938], Loss: 0.0629\n",
      "Epoch [11/12], Step [800/938], Loss: 0.0345\n",
      "Epoch [11/12], Step [900/938], Loss: 0.0702\n",
      "\tLearningRate:  0.0010000000000000002\n",
      "Epoch [12/12], Step [100/938], Loss: 0.0857\n",
      "Epoch [12/12], Step [200/938], Loss: 0.1099\n",
      "Epoch [12/12], Step [300/938], Loss: 0.1268\n",
      "Epoch [12/12], Step [400/938], Loss: 0.0855\n",
      "Epoch [12/12], Step [500/938], Loss: 0.1282\n",
      "Epoch [12/12], Step [600/938], Loss: 0.1438\n",
      "Epoch [12/12], Step [700/938], Loss: 0.1845\n",
      "Epoch [12/12], Step [800/938], Loss: 0.2291\n",
      "Epoch [12/12], Step [900/938], Loss: 0.0764\n",
      "\tLearningRate:  0.0010000000000000002\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class CNN_2(nn.Module): # Definimos la red neuronal convolucional\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1) # primera capa convolucional, 1 canal de color, 32 features, ventana de 3x3. Tamaño 32*28*28\n",
    "    self.pool = nn.MaxPool2d(2, 2) # capa de maxpooling, reduce el tamaño de la imagen a la mitad manteniendo las features\n",
    "    self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1) # segunda capa convolucional, 32 features de entrada, 64 de salida, ventana de 3x3. Tamaño de la imagen 64*14*14\n",
    "    self.fc1 = nn.Linear(7 * 7 * 64, 128) # capa linear de entrada, toma un tensor aplanado de 7*7*64 en la entrada y devuelve 128 en la salida\n",
    "    self.fc2 = nn.Linear(128, 10) # capa linear de salida, toma un tensor de 128 en la entrada y devuelve 10 en la salida que son el número de etiquetas del dataset\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.pool(F.relu(self.conv1(x))) # paso por 1ª capa convolucional del batch, aplica función de activación Relu y reduce tamaño con maxpooling (1,28,28->32,14,14)\n",
    "    x = self.pool(F.relu(self.conv2(x))) # paso por 2ª capa convolucional la salida de la 1ª capa, aplica función de activación Relu y reduce tamaño con maxpooling (32,14,14->64,7,7)\n",
    "    x = x.view(-1, 7 * 7 * 64) # Aplanamiento tras las capas convolucionales (flatten)\n",
    "    x = F.sigmoid(self.fc1(x)) # paso por 1ª capa linear de la red neuronal, 3136 neuronas de entrada y 128 de salida, aplica funcion de activación Relu (3136,128)\n",
    "    x = F.log_softmax(self.fc2(x), dim=1)  # paso por 2ª capa de la red neuronal, 128 neuronas de entrada y 10 de salida, correspondientes con las etiquetas del dataset.\n",
    "    return x\n",
    "  \n",
    "modelConv_2 = CNN_2() # Instanciamos la red neuronal\n",
    "\n",
    "lossFn = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.SGD(modelConv_2.parameters(), lr=0.1, weight_decay=1e-6, momentum=0.9)\n",
    "\n",
    "# Learning Rate Annealing (LRA) scheduling\n",
    "# lr = 0.1     if epoch < 25\n",
    "# lr = 0.01    if 25 <= epoch < 50\n",
    "# lr = 0.001   if epoch >= 50\n",
    "sched = torch.optim.lr_scheduler.MultiStepLR(optim, milestones=[4, 8], gamma=0.1)\n",
    "\n",
    "\n",
    "\n",
    "modelConv_2.train() # Ponemos el modelo en modo entrenamiento \n",
    "\n",
    "for epoch in range(12): # Definimos 10 epochs\n",
    "  \n",
    "  for i, (images, labels) in enumerate(train_Floader):\n",
    "    # Forward pass\n",
    "    outputs = modelConv_2(images) # Genera predicciones\n",
    "    loss = lossFn(outputs, labels) # Calcula la perdida en la iteración\n",
    "\n",
    "    # Backpropagation\n",
    "    optim.zero_grad() # Resetea los gradientes\n",
    "    loss.backward() # Calcula el gradiente de la función de pérdida\n",
    "    optim.step() # Actualiza los parámetros de pesos y sesgos\n",
    "    \n",
    "    \n",
    "\n",
    "    if (i + 1) % 100 == 0:\n",
    "      print(f'Epoch [{epoch+1}/{12}], Step [{i+1}/{len(train_Floader)}], Loss: {loss.item():.4f}')\n",
    "  sched.step()\n",
    "  print(\"\\tLearningRate: \", optim.param_groups[0]['lr'])    \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluación del modelo modificado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 92.13%\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(): # Deshabilita el calculo de gradientes\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  for images, labels in test_Floader:\n",
    "    outputs = modelConv_2(images)\n",
    "    _, pred = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (pred == labels).sum().item()\n",
    "  print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comprobamos que cambiando el optimizador a uno con tasa de aprendizaje variable y añadiendo un par de Epochs y modificando la función de activación a Sigmoide, la Exactitud mejora casi un  punto hasta el 92,13%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
